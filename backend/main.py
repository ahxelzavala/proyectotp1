from fastapi import FastAPI, File, UploadFile, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from sqlalchemy import text, func
import pandas as pd
import io
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
import traceback
import json
import uvicorn
import os
import re
from pathlib import Path

# Importar modelos y configuraciÃ³n
from models import get_database, ClientData, AuthorizedEmail, create_tables, test_database_connection, migrate_add_new_columns
from config import settings

from auth import (
    get_password_hash, 
    verify_password, 
    create_access_token, 
    authenticate_user,
    validate_email_domain,
    get_current_user,
    get_current_admin_user
)
from models import User, UserRole, UserStatus
from pydantic import BaseModel, EmailStr
from typing import Optional, List

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===== IMPORTS PARA ML =====
# ===== IMPORTS PARA ML CON MANEJO ROBUSTO =====
import traceback
from datetime import datetime
import random

# Intentar importar ml_service real
try:
    from ml_service import ml_service
    ML_AVAILABLE = True
    logger.info("âœ… ML Service importado correctamente")
    logger.info(f"ðŸ¤– Modo: {'REAL' if not ml_service.demo_mode else 'DEMO'}")
    
except ImportError as e:
    logger.warning(f"âš ï¸ Import fallido: {e}")
    logger.info("ðŸ“¦ Usando MockMLService...")
    ML_AVAILABLE = False
    
    # MockMLService COMPLETO
    class MockMLService:
        def __init__(self):
            self.is_loaded = True
            self.demo_mode = True
            self.model = None
            self.model_metadata = {
                'threshold': 0.5,
                'model_version': 'Demo v1.0',
                'training_date': datetime.now().strftime("%Y-%m-%d"),
                'metrics': {
                    'accuracy': 0.8008,
                    'precision': 0.6861,
                    'recall': 0.5729,
                    'f1_score': 0.6244,
                    'roc_auc': 0.8466
                }
            }
            logger.info("âœ… MockMLService listo")
        
        def get_model_info(self):
            return {
                "loaded": True,
                "demo_mode": True,
                "model_version": "Demo v1.0",
                "metrics": self.model_metadata['metrics'],
                "message": "Modo demo activo"
            }
        
        def predict_cross_sell(self, client_data, threshold=None):
            """Predicciones usando datos reales"""
            threshold = threshold or 0.5
            results = []
            
            for i, client in enumerate(client_data):
                venta = float(client.get('venta', 0))
                mb = float(client.get('mb', 0))
                
                # Calcular probabilidad por reglas
                prob = 0.3
                if venta > 50000: prob += 0.25
                elif venta > 20000: prob += 0.15
                elif venta > 5000: prob += 0.10
                
                if mb > 0 and venta > 0:
                    rent = mb / venta
                    if rent > 0.3: prob += 0.20
                    elif rent > 0.15: prob += 0.10
                
                # VariaciÃ³n por cliente
                random.seed(hash(str(client.get('cliente', ''))) % 10000)
                prob += random.uniform(-0.05, 0.05)
                prob = max(0.05, min(0.95, prob))
                
                pred = 1 if prob >= threshold else 0
                priority = "Alta" if prob >= 0.7 else ("Media" if prob >= 0.5 else "Baja")
                
                results.append({
                    "client_id": client.get('id', i),
                    "client_name": client.get('cliente', 'Sin nombre'),
                    "probability": round(prob, 4),
                    "prediction": pred,
                    "recommendation": "SÃ­" if pred == 1 else "No",
                    "priority": priority,
                    "venta_actual": venta,
                    "venta": venta,
                    "mb_total": mb,
                    "mb": mb,
                    "tipo_cliente": client.get('tipo_de_cliente', 'Sin tipo'),
                    "categoria": client.get('categoria', 'Sin categorÃ­a'),
                    "comercial": client.get('comercial', 'Sin asignar'),
                    "demo_mode": True
                })
            
            return results
        
        def get_feature_importance(self):
            return [
                {"feature": "Venta", "importance": 0.30},
                {"feature": "Rentabilidad", "importance": 0.25},
                {"feature": "Tipo Cliente", "importance": 0.20}
            ]
    
    ml_service = MockMLService()

except Exception as e:
    logger.error(f"âŒ Error crÃ­tico: {e}")
    logger.error(traceback.format_exc())
    
    class EmergencyService:
        def __init__(self):
            self.is_loaded = False
            self.demo_mode = True
            self.model_metadata = {'threshold': 0.5}
        def get_model_info(self): return {"loaded": False}
        def predict_cross_sell(self, *args, **kwargs): return []
        def get_feature_importance(self): return []
    
    ml_service = EmergencyService()
    ML_AVAILABLE = False

# Log estado final
logger.info("="*50)
logger.info(f"ðŸ¤– ML SERVICE: {type(ml_service).__name__}")
logger.info(f"   Cargado: {ml_service.is_loaded}")
logger.info(f"   Demo: {ml_service.demo_mode}")
logger.info("="*50)

# ===== MODELOS PYDANTIC PARA ML =====
from pydantic import BaseModel

class PredictionRequest(BaseModel):
    client_ids: Optional[List[int]] = None
    threshold: Optional[float] = None
    limit: Optional[int] = 100

class SingleClientPrediction(BaseModel):
    cliente: str
    venta: float
    costo: float
    mb: float
    cantidad: float
    tipo_de_cliente: str
    categoria: str
    comercial: Optional[str] = None
    proveedor: Optional[str] = None
    threshold: Optional[float] = None



app = FastAPI(
    title="Sistema de AnÃ¡lisis Anders",
    description="API para importar y analizar datos CSV completos",
    version="2.0.0"
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:5173",
        "https://proyectotp1.vercel.app",
        "https://proyectotp2.vercel.app",
        "https://proyectotp11.vercel.app",
        "https://proyectotp10.vercel.app",
        "https://proyectotp16.vercel.app",
        "https://proyectotp22.vercel.app",
        "*",  # Temporal para debugging
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
@app.get("/")
def read_root():
    return {"message": "Sistema Anders API", "status": "running"}

@app.get("/health")
def health_check():
    return {"status": "healthy"}


# ===== STARTUP EVENT =====
@app.on_event("startup")
async def startup_event():
    """Inicializar la base de datos al arrancar"""
    logger.info("Iniciando aplicaciÃ³n...")
    
    if not test_database_connection():
        logger.error("âŒ No se pudo conectar a la base de datos")
        raise Exception("Error de conexiÃ³n a la base de datos")
    
    if not create_tables():
        logger.error("âŒ No se pudieron crear las tablas")
        raise Exception("Error creando tablas de la base de datos")
    
    # Ejecutar migraciÃ³n para agregar nuevas columnas
    if not migrate_add_new_columns():
        logger.warning("âš ï¸ No se pudieron agregar todas las columnas nuevas")
    
    # Verificar ml Service
    if ML_AVAILABLE and ml_service.is_loaded:
        logger.info("âœ… Sistema ML inicializado correctamente")
    else:
        logger.info("âš ï¸ Sistema funcionando sin ML (usar modo demo)")
    
    logger.info("âœ… Base de datos inicializada correctamente")

@app.get("/")
async def root():
    return {"message": "Bienvenido al Sistema de AnÃ¡lisis Anders v2.0 - CSV Completo"}

@app.get("/health")
async def health_check():
    try:
        db = next(get_database())
        db.execute(text("SELECT 1"))
        return {"status": "ok", "database": "connected"}
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {"status": "error", "database": "disconnected", "error": str(e)}

# ===== ENDPOINT DE DIAGNÃ“STICO =====
@app.get("/debug/data-status")
async def debug_data_status_postgresql(db: Session = Depends(get_database)):
    """DiagnÃ³stico especÃ­fico para PostgreSQL"""
    try:
        logger.info("ðŸ” Ejecutando diagnÃ³stico de PostgreSQL...")
        
        # Verificar conexiÃ³n
        db.execute(text("SELECT 1")).scalar()
        logger.info("âœ… ConexiÃ³n a PostgreSQL OK")
        
        # Verificar si existe la tabla
        table_exists_query = text("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public'
                AND table_name = 'client_data'
            )
        """)
        table_exists = db.execute(table_exists_query).scalar()
        logger.info(f"ðŸ“‹ Tabla existe: {table_exists}")
        
        if not table_exists:
            return {
                "error": "Tabla client_data no existe en el esquema public",
                "solution": "Ejecutar migraciÃ³n de base de datos",
                "table_exists": False,
                "postgres_specific": True
            }
        
        # Contar registros
        total_count = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar()
        logger.info(f"ðŸ“Š Total registros: {total_count}")
        
        # Verificar estructura de datos
        sample_query = text("""
            SELECT 
                COUNT(*) as total_rows,
                COUNT(cliente) as cliente_not_null,
                COUNT(venta) as venta_not_null,
                COUNT(mb) as mb_not_null
            FROM client_data
        """)
        
        sample_result = db.execute(sample_query).fetchone()
        
        # Muestra de datos
        sample_data_query = text("""
            SELECT cliente, venta, mb, factura, fecha
            FROM client_data 
            WHERE cliente IS NOT NULL 
            LIMIT 3
        """)
        
        sample_data = db.execute(sample_data_query).fetchall()
        
        # Verificar tipos de columnas
        column_types_query = text("""
            SELECT 
                column_name, 
                data_type,
                is_nullable
            FROM information_schema.columns 
            WHERE table_schema = 'public'
            AND table_name = 'client_data' 
            AND column_name IN ('venta', 'mb', 'costo', 'cantidad')
            ORDER BY column_name
        """)
        
        column_types = db.execute(column_types_query).fetchall()
        
        return {
            "success": True,
            "postgres_version": "Compatible",
            "table_exists": table_exists,
            "total_records": total_count,
            "data_quality": {
                "total_rows": sample_result.total_rows if sample_result else 0,
                "cliente_not_null": sample_result.cliente_not_null if sample_result else 0,
                "venta_not_null": sample_result.venta_not_null if sample_result else 0,
                "mb_not_null": sample_result.mb_not_null if sample_result else 0
            },
            "sample_data": [
                {
                    "cliente": row.cliente,
                    "venta": str(row.venta),
                    "mb": str(row.mb),
                    "factura": row.factura,
                    "fecha": row.fecha
                }
                for row in sample_data
            ],
            "column_info": [
                {
                    "column": row.column_name,
                    "type": row.data_type,
                    "nullable": row.is_nullable
                }
                for row in column_types
            ],
            "recommendations": [
                "Usar ::text y ::numeric para conversiones seguras",
                "Verificar que los datos numÃ©ricos no contengan caracteres especiales",
                "Considerar usar CAST() en lugar de conversiones implÃ­citas"
            ]
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en diagnÃ³stico: {str(e)}")
        return {
            "error": f"Error en diagnÃ³stico PostgreSQL: {str(e)}",
            "success": False,
            "traceback": traceback.format_exc(),
            "postgres_specific": True
        }

# ===== ENDPOINT DE MÃ‰TRICAS CORREGIDO =====
@app.get("/analytics/summary")
async def get_summary_analytics_postgresql(db: Session = Depends(get_database)):
    """Obtener mÃ©tricas reales adaptadas especÃ­ficamente para PostgreSQL"""
    try:
        logger.info("ðŸ” Iniciando cÃ¡lculo de mÃ©tricas para PostgreSQL...")
        
        # Verificar que la tabla existe
        table_check_query = text("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public'
                AND table_name = 'client_data'
            )
        """)
        table_exists = db.execute(table_check_query).scalar()
        
        if not table_exists:
            logger.error("âŒ Tabla client_data no existe")
            return {
                "success": False,
                "error": "Tabla client_data no existe en la base de datos",
                "summary": {
                    "total_records": 0,
                    "unique_clients": 0,
                    "total_sales": 0.0,
                    "average_margin_percentage": 0.0
                }
            }
        
        # Contar registros totales
        total_records = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar() or 0
        logger.info(f"ðŸ“Š Total de registros: {total_records}")
        
        if total_records == 0:
            logger.warning("âš ï¸ No hay datos en la tabla")
            return {
                "success": False,
                "message": "No hay datos en la tabla client_data. Por favor, sube un archivo CSV.",
                "summary": {
                    "total_records": 0,
                    "unique_clients": 0,
                    "total_sales": 0.0,
                    "average_margin_percentage": 0.0
                }
            }
        
        # Clientes Ãºnicos - PostgreSQL compatible
        unique_clients_query = text("""
            SELECT COUNT(DISTINCT cliente) 
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
        """)
        unique_clients = db.execute(unique_clients_query).scalar() or 0
        logger.info(f"ðŸ‘¥ Clientes Ãºnicos: {unique_clients}")
        
        # Ventas totales - ConversiÃ³n segura para PostgreSQL
        total_sales_query = text("""
            SELECT COALESCE(
                SUM(
                    CASE 
                        WHEN venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ), 0
            )
            FROM client_data 
            WHERE venta IS NOT NULL
        """)
        total_sales = float(db.execute(total_sales_query).scalar() or 0)
        logger.info(f"ðŸ’° Ventas totales: {total_sales}")
        
        # Margen bruto total - PostgreSQL compatible
        total_margin_query = text("""
            SELECT COALESCE(
                SUM(
                    CASE 
                        WHEN mb::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN mb::numeric
                        ELSE 0
                    END
                ), 0
            )
            FROM client_data 
            WHERE mb IS NOT NULL
        """)
        total_margin = float(db.execute(total_margin_query).scalar() or 0)
        logger.info(f"ðŸ“ˆ Margen total: {total_margin}")
        
        # Calcular margen promedio
        average_margin_percentage = 0
        if total_sales > 0:
            average_margin_percentage = round((total_margin / total_sales) * 100, 2)
        
        # MÃ©tricas adicionales
        unique_invoices = db.execute(text("""
            SELECT COUNT(DISTINCT factura) 
            FROM client_data 
            WHERE factura IS NOT NULL AND TRIM(factura) != ''
        """)).scalar() or 0
        
        unique_products = db.execute(text("""
            SELECT COUNT(DISTINCT articulo) 
            FROM client_data 
            WHERE articulo IS NOT NULL AND TRIM(articulo) != ''
        """)).scalar() or 0
        
        # CÃ¡lculos derivados
        average_transaction_value = round(total_sales / unique_invoices, 2) if unique_invoices > 0 else 0
        average_sales_per_client = round(total_sales / unique_clients, 2) if unique_clients > 0 else 0
        
        # Top 3 clientes para validaciÃ³n
        top_clients_query = text("""
            SELECT 
                cliente,
                SUM(
                    CASE 
                        WHEN venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) as total_venta
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
            GROUP BY cliente
            ORDER BY total_venta DESC
            LIMIT 3
        """)
        
        top_clients_result = db.execute(top_clients_query).fetchall()
        top_clients = [
            {"cliente": row.cliente, "total_venta": float(row.total_venta)}
            for row in top_clients_result
        ]
        
        logger.info("âœ… MÃ©tricas calculadas exitosamente")
        
        return {
            "success": True,
            "message": "MÃ©tricas calculadas exitosamente desde datos reales",
            "data_source": "CSV cargado en client_data (PostgreSQL)",
            "summary": {
                "total_records": total_records,
                "unique_clients": unique_clients,
                "total_sales": total_sales,
                "total_margin": total_margin,
                "average_margin_percentage": average_margin_percentage,
                "unique_invoices": unique_invoices,
                "unique_products": unique_products,
                "average_transaction_value": average_transaction_value,
                "average_sales_per_client": average_sales_per_client
            },
            "top_clients": top_clients,
            "explanations": {
                "total_records": "NÃºmero total de transacciones en el CSV cargado",
                "unique_clients": "Clientes Ãºnicos identificados en el campo 'Cliente'",
                "total_sales": "Suma total de ventas vÃ¡lidas del campo 'Venta'",
                "average_margin_percentage": "Margen bruto promedio: (Total MB Ã· Total Ventas) Ã— 100"
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en mÃ©tricas: {str(e)}")
        logger.error(f"Traceback completo: {traceback.format_exc()}")
        
        # Fallback bÃ¡sico
        try:
            basic_count = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar() or 0
            return {
                "success": False,
                "error": f"Error calculando mÃ©tricas: {str(e)}",
                "debug_info": str(e),
                "summary": {
                    "total_records": basic_count,
                    "unique_clients": 0,
                    "total_sales": 0.0,
                    "total_margin": 0.0,
                    "average_margin_percentage": 0.0
                }
            }
        except:
            return {
                "success": False,
                "error": "Error crÃ­tico accediendo a los datos",
                "summary": {
                    "total_records": 0,
                    "unique_clients": 0,
                    "total_sales": 0.0,
                    "total_margin": 0.0,
                    "average_margin_percentage": 0.0
                }
            }

@app.post("/upload-csv")
async def upload_csv(
    file: UploadFile = File(...),
    replace_data: bool = True,
    db: Session = Depends(get_database)
):
    """
    Endpoint mejorado para cargar CSV con TODAS las columnas
    """
    try:
        logger.info(f"Procesando archivo completo: {file.filename}")
        
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="El archivo debe ser un CSV (.csv)")
        
        # Limpiar datos anteriores si se solicita
        if replace_data:
            try:
                deleted_count = db.query(ClientData).delete()
                db.commit()
                logger.info(f"ðŸ—‘ï¸ Datos anteriores eliminados: {deleted_count} registros")
            except Exception as e:
                db.rollback()
                logger.error(f"Error eliminando datos anteriores: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Error limpiando datos anteriores: {str(e)}")
        
        # Leer el archivo
        contents = await file.read()
        csv_string = contents.decode('utf-8')
        csv_io = io.StringIO(csv_string)
        
        # Leer CSV
        try:
            df = pd.read_csv(
                csv_io,
                encoding='utf-8',
                skipinitialspace=True,
                na_values=['', 'NA', 'N/A', 'null', 'NULL', 'None', 'NONE'],
                keep_default_na=True
            )
            logger.info(f"CSV leÃ­do exitosamente. Filas: {len(df)}, Columnas: {len(df.columns)}")
            logger.info(f"Columnas encontradas: {list(df.columns)}")
        except Exception as e:
            logger.error(f"Error leyendo CSV: {str(e)}")
            raise HTTPException(status_code=400, detail=f"Error al leer el archivo CSV: {str(e)}")
        
        if df.empty:
            raise HTTPException(status_code=400, detail="El archivo CSV estÃ¡ vacÃ­o")
        
        # FunciÃ³n para obtener valor de manera segura
        def get_safe_value(row, column_name, convert_to=str, default=None):
            """Obtener valor de columna de manera segura"""
            if column_name in df.columns and not pd.isna(row[column_name]):
                try:
                    value = row[column_name]
                    if convert_to == str:
                        return str(value).strip() if str(value).strip() not in ['nan', 'None', 'null'] else default
                    elif convert_to == float:
                        # Limpiar valor numÃ©rico
                        if isinstance(value, (int, float)):
                            return float(value)
                        else:
                            value_str = str(value).replace(',', '').replace('$', '').replace('%', '').strip()
                            import re
                            value_str = re.sub(r'[^\d.-]', '', value_str)
                            return float(value_str) if value_str else default
                    elif convert_to == datetime:
                        parsed_date = pd.to_datetime(value, errors='coerce')
                        return parsed_date.to_pydatetime() if not pd.isna(parsed_date) else default
                except Exception as e:
                    logger.warning(f"Error convirtiendo {column_name}: {e}")
                    return default
            return default
        
        # Procesar registros
        processed_records = []
        errors = []
        
        logger.info(f"Procesando {len(df)} filas con todas las columnas...")
        
        for index, row in df.iterrows():
            try:
                # Crear registro con TODAS las columnas del CSV
                client_record = ClientData(
                    # Metadatos
                    uploaded_at=datetime.utcnow(),
                    filename=file.filename,
                    
                    # ===== TODAS LAS COLUMNAS DEL CSV =====
                    fecha=get_safe_value(row, 'Fecha', str),
                    tipo_de_venta=get_safe_value(row, 'Tipo de Venta', str),
                    documento=get_safe_value(row, 'Documento', str),
                    factura=get_safe_value(row, 'Factura', str),
                    codigo=get_safe_value(row, 'Codigo', str),
                    cliente=get_safe_value(row, 'Cliente', str),
                    tipo_de_cliente=get_safe_value(row, 'Tipo de Cliente', str),
                    sku=get_safe_value(row, 'SKU', str),
                    articulo=get_safe_value(row, 'Articulo', str),
                    proveedor=get_safe_value(row, 'Proveedor', str),
                    almacen=get_safe_value(row, 'Almacen', str),
                    cantidad=get_safe_value(row, 'Cantidad', float, 0.0),
                    um=get_safe_value(row, 'U.M.', str),
                    p_venta=get_safe_value(row, 'P. Venta', float, 0.0),
                    c_unit=get_safe_value(row, 'C. Unit', float, 0.0),
                    venta=get_safe_value(row, 'Venta', float, 0.0),
                    costo=get_safe_value(row, 'Costo', float, 0.0),
                    mb=get_safe_value(row, 'MB', float, 0.0),
                    mb_percent=get_safe_value(row, '%MB', str),
                    sociedad=get_safe_value(row, 'Sociedad', str),
                    bc=get_safe_value(row, 'BC', str),
                    bt=get_safe_value(row, 'BT', str),
                    bu=get_safe_value(row, 'BU', str),
                    bs=get_safe_value(row, 'BS', str),
                    comercial=get_safe_value(row, 'Comercial', str),
                    tipo_cliente=get_safe_value(row, 'Tipo_Cliente', str),
                    categoria=get_safe_value(row, 'CATEGORIA', str),
                    supercategoria=get_safe_value(row, 'SUPERCATEGORIA', str),
                    cruce=get_safe_value(row, 'CRUCE', str),
                    
                    # ===== CAMPOS DE COMPATIBILIDAD =====
                    client_name=get_safe_value(row, 'Cliente', str, "Sin nombre"),
                    client_type=get_safe_value(row, 'Tipo de Cliente', str, "No especificado"),
                    executive=get_safe_value(row, 'Comercial', str, "No asignado"),
                    product=get_safe_value(row, 'Articulo', str, "No especificado"),
                    value=get_safe_value(row, 'Venta', float, 0.0),
                    date=get_safe_value(row, 'Fecha', datetime, datetime.utcnow()),
                    description=f"Importado desde {file.filename} - Fila {index + 1}"
                )
                
                processed_records.append(client_record)
                
                if (index + 1) % 1000 == 0:
                    logger.info(f"Procesadas {index + 1} filas...")
                    
            except Exception as e:
                logger.error(f"Error procesando fila {index + 1}: {str(e)}")
                errors.append(f"Fila {index + 1}: Error procesando datos - {str(e)}")
                continue
        
        if not processed_records:
            error_message = f"No se pudieron procesar registros. Errores: {'; '.join(errors[:5])}"
            logger.error(error_message)
            raise HTTPException(status_code=400, detail=error_message)
        
        logger.info(f"Registros procesados exitosamente: {len(processed_records)}")
        
        # Guardar en lotes
        saved_count = 0
        try:
            batch_size = 1000
            total_batches = (len(processed_records) + batch_size - 1) // batch_size
            
            for i in range(0, len(processed_records), batch_size):
                batch = processed_records[i:i + batch_size]
                db.add_all(batch)
                db.commit()
                saved_count += len(batch)
                logger.info(f"Lote {(i // batch_size) + 1}/{total_batches} guardado: {len(batch)} registros")
            
            logger.info(f"âœ… {saved_count} registros guardados exitosamente con todas las columnas")
            
        except Exception as e:
            db.rollback()
            error_msg = f"Error guardando en base de datos: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            raise HTTPException(status_code=500, detail=error_msg)
        
        # Preparar respuesta detallada
        response_data = {
            "success": True,
            "message": f"Archivo procesado exitosamente con todas las columnas. {saved_count} registros guardados.",
            "details": {
                "filename": file.filename,
                "total_rows": len(df),
                "processed_rows": len(processed_records),
                "saved_rows": saved_count,
                "errors_count": len(errors),
                "all_columns_mapped": True,
                "columns_found": list(df.columns),
                "columns_count": len(df.columns),
                "storage_method": "Todas las columnas en campos individuales"
            }
        }
        
        if errors:
            response_data["errors"] = errors[:10]
            if len(errors) > 10:
                response_data["errors"].append(f"... y {len(errors) - 10} errores mÃ¡s")
        
        logger.info(f"Respuesta enviada: {response_data['message']}")
        return JSONResponse(content=response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = f"Error inesperado: {str(e)}"
        logger.error(error_msg)
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/client-data")
async def get_client_data(
    limit: int = 100,
    offset: int = 0,
    include_all_fields: bool = False,
    db: Session = Depends(get_database)
):
    """Obtener datos de clientes con opciÃ³n de incluir todos los campos"""
    try:
        total_count = db.query(ClientData).count()
        clients = db.query(ClientData).offset(offset).limit(limit).all()
        
        logger.info(f"Consultando datos: {len(clients)} registros (de {total_count} totales)")
        
        if include_all_fields:
            # Devolver TODOS los campos
            client_data = []
            for client in clients:
                client_dict = {
                    "id": client.id,
                    "uploaded_at": client.uploaded_at.isoformat() if client.uploaded_at else None,
                    "filename": client.filename,
                    
                    # Campos del CSV
                    "fecha": client.fecha,
                    "tipo_de_venta": client.tipo_de_venta,
                    "documento": client.documento,
                    "factura": client.factura,
                    "codigo": client.codigo,
                    "cliente": client.cliente,
                    "tipo_de_cliente": client.tipo_de_cliente,
                    "sku": client.sku,
                    "articulo": client.articulo,
                    "proveedor": client.proveedor,
                    "almacen": client.almacen,
                    "cantidad": float(client.cantidad) if client.cantidad else None,
                    "um": client.um,
                    "p_venta": float(client.p_venta) if client.p_venta else None,
                    "c_unit": float(client.c_unit) if client.c_unit else None,
                    "venta": float(client.venta) if client.venta else None,
                    "costo": float(client.costo) if client.costo else None,
                    "mb": float(client.mb) if client.mb else None,
                    "mb_percent": client.mb_percent,
                    "sociedad": client.sociedad,
                    "bc": client.bc,
                    "bt": client.bt,
                    "bu": client.bu,
                    "bs": client.bs,
                    "comercial": client.comercial,
                    "tipo_cliente": client.tipo_cliente,
                    "categoria": client.categoria,
                    "supercategoria": client.supercategoria,
                    "cruce": client.cruce,
                    
                    # Campos de compatibilidad
                    "client_name": client.client_name,
                    "client_type": client.client_type,
                    "executive": client.executive,
                    "product": client.product,
                    "value": client.value,
                    "date": client.date.isoformat() if client.date else None,
                    "description": client.description
                }
                client_data.append(client_dict)
        else:
            # Devolver solo campos bÃ¡sicos para compatibilidad
            client_data = [
                {
                    "id": client.id,
                    "client_name": client.client_name,
                    "client_type": client.client_type,
                    "executive": client.executive,
                    "product": client.product,
                    "value": client.value,
                    "date": client.date.isoformat() if client.date else None,
                    "description": client.description,
                    
                    # Campos principales del CSV
                    "cliente": client.cliente,
                    "factura": client.factura,
                    "venta": float(client.venta) if client.venta else None,
                    "fecha": client.fecha,
                    "comercial": client.comercial,
                    "categoria": client.categoria
                }
                for client in clients
            ]
        
        return {
            "success": True,
            "total_count": total_count,
            "count": len(clients),
            "offset": offset,
            "limit": limit,
            "include_all_fields": include_all_fields,
            "data": client_data
        }
    except Exception as e:
        logger.error(f"Error consultando datos: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/client-data/full")
async def get_client_data_full(
    limit: int = 100,
    offset: int = 0,
    db: Session = Depends(get_database)
):
    """Obtener datos completos con todas las columnas del CSV"""
    return await get_client_data(limit, offset, include_all_fields=True, db=db)

@app.get("/client-data/search")
async def search_client_data(
    cliente: str = None,
    factura: str = None,
    fecha_desde: str = None,
    fecha_hasta: str = None,
    comercial: str = None,
    categoria: str = None,
    proveedor: str = None,
    limit: int = 100,
    offset: int = 0,
    db: Session = Depends(get_database)
):
    """Buscar datos con filtros especÃ­ficos"""
    try:
        query = db.query(ClientData)
        
        # Aplicar filtros
        if cliente:
            query = query.filter(ClientData.cliente.ilike(f'%{cliente}%'))
        
        if factura:
            query = query.filter(ClientData.factura.ilike(f'%{factura}%'))
        
        if fecha_desde:
            query = query.filter(ClientData.fecha >= fecha_desde)
        
        if fecha_hasta:
            query = query.filter(ClientData.fecha <= fecha_hasta)
        
        if comercial:
            query = query.filter(ClientData.comercial.ilike(f'%{comercial}%'))
        
        if categoria:
            query = query.filter(ClientData.categoria.ilike(f'%{categoria}%'))
        
        if proveedor:
            query = query.filter(ClientData.proveedor.ilike(f'%{proveedor}%'))
        
        total_count = query.count()
        records = query.offset(offset).limit(limit).all()
        
        return {
            "success": True,
            "total_count": total_count,
            "count": len(records),
            "filters_applied": {
                "cliente": cliente,
                "factura": factura,
                "fecha_desde": fecha_desde,
                "fecha_hasta": fecha_hasta,
                "comercial": comercial,
                "categoria": categoria,
                "proveedor": proveedor
            },
            "data": [
                {
                    "id": record.id,
                    "cliente": record.cliente,
                    "factura": record.factura,
                    "fecha": record.fecha,
                    "venta": float(record.venta) if record.venta else None,
                    "comercial": record.comercial,
                    "categoria": record.categoria,
                    "proveedor": record.proveedor,
                    "articulo": record.articulo,
                    "cantidad": float(record.cantidad) if record.cantidad else None,
                    "um": record.um
                }
                for record in records
            ]
        }
        
    except Exception as e:
        logger.error(f"Error en bÃºsqueda: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/client-data/clear")
async def clear_client_data(db: Session = Depends(get_database)):
    """Limpiar todos los datos de clientes"""
    try:
        deleted_count = db.query(ClientData).delete()
        db.commit()
        logger.info(f"Se eliminaron {deleted_count} registros")
        return {
            "success": True,
            "message": f"Se eliminaron {deleted_count} registros"
        }
    except Exception as e:
        db.rollback()
        logger.error(f"Error eliminando datos: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/debug/count")
async def debug_count(db: Session = Depends(get_database)):
    """Endpoint de debug para verificar el conteo de registros"""
    try:
        count = db.query(ClientData).count()
        
        # Obtener una muestra de los primeros 3 registros con todos los campos
        sample = db.query(ClientData).limit(3).all()
        sample_data = []
        
        for client in sample:
            sample_data.append({
                "id": client.id,
                "cliente": client.cliente,
                "factura": client.factura,
                "fecha": client.fecha,
                "venta": float(client.venta) if client.venta else None,
                "comercial": client.comercial,
                "categoria": client.categoria,
                "articulo": client.articulo,
                "proveedor": client.proveedor,
                "filename": client.filename
            })
        
        return {
            "count": count, 
            "message": f"Hay {count} registros en la base de datos",
            "sample_data": sample_data,
            "all_columns_available": True
        }
    except Exception as e:
        logger.error(f"Error en debug count: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/debug/columns")
async def debug_columns():
    """Endpoint para mostrar todas las columnas disponibles"""
    return {
        "csv_columns_mapped": [
            "fecha", "tipo_de_venta", "documento", "factura", "codigo", "cliente", 
            "tipo_de_cliente", "sku", "articulo", "proveedor", "almacen", "cantidad", 
            "um", "p_venta", "c_unit", "venta", "costo", "mb", "mb_percent", 
            "sociedad", "bc", "bt", "bu", "bs", "comercial", "tipo_cliente", 
            "categoria", "supercategoria", "cruce"
        ],
        "compatibility_fields": [
            "client_name", "client_type", "executive", "product", "value", "date", "description"
        ],
        "metadata_fields": [
            "id", "uploaded_at", "filename"
        ],
        "total_fields": 32,
        "message": "Todas las columnas del CSV se mapean a campos individuales de la tabla"
    }

@app.post("/preview-csv")
async def preview_csv(file: UploadFile = File(...)):
    """Previsualizar un CSV sin guardarlo"""
    try:
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="El archivo debe ser un CSV (.csv)")
        
        contents = await file.read()
        csv_string = contents.decode('utf-8')
        csv_io = io.StringIO(csv_string)
        
        # Leer solo las primeras 10 filas
        df = pd.read_csv(
            csv_io,
            encoding='utf-8',
            skipinitialspace=True,
            na_values=['', 'NA', 'N/A', 'null', 'NULL', 'None', 'NONE'],
            keep_default_na=True,
            nrows=10
        )
        
        original_columns = list(df.columns)
        
        # Verificar mapeo de columnas
        expected_columns = [
            "Fecha", "Tipo de Venta", "Documento", "Factura", "Codigo", "Cliente", 
            "Tipo de Cliente", "SKU", "Articulo", "Proveedor", "Almacen", "Cantidad", 
            "U.M.", "P. Venta", "C. Unit", "Venta", "Costo", "MB", "%MB", 
            "Sociedad", "BC", "BT", "BU", "BS", "Comercial", "Tipo_Cliente", 
            "CATEGORIA", "SUPERCATEGORIA", "CRUCE"
        ]
        
        matched_columns = [col for col in expected_columns if col in original_columns]
        missing_columns = [col for col in expected_columns if col not in original_columns]
        extra_columns = [col for col in original_columns if col not in expected_columns]
        
        # Convertir muestra a JSON
        sample_data = []
        for index, row in df.iterrows():
            row_dict = {}
            for col in df.columns:
                value = row[col]
                if pd.isna(value):
                    row_dict[col] = None
                else:
                    row_dict[col] = str(value)
            sample_data.append(row_dict)
        
        return {
            "success": True,
            "filename": file.filename,
            "columns_found": original_columns,
            "columns_count": len(original_columns),
            "matching_analysis": {
                "matched_columns": matched_columns,
                "missing_columns": missing_columns,
                "extra_columns": extra_columns,
                "match_percentage": round((len(matched_columns) / len(expected_columns)) * 100, 2)
            },
            "sample_data": sample_data,
            "total_rows_preview": len(df),
            "ready_to_upload": len(missing_columns) == 0,
            "message": f"CSV analizado: {len(matched_columns)}/{len(expected_columns)} columnas coinciden"
        }
        
    except Exception as e:
        logger.error(f"Error en preview CSV: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error procesando archivo: {str(e)}")

# ===== ENDPOINTS ML CORREGIDOS =====

@app.get("/ml/status")
async def get_ml_status():
    """Verificar el estado del modelo de ML"""
    try:
        model_info = ml_service.get_model_info()
        return {
            "success": True,
            "model_info": model_info,
            "message": "Modelo cargado correctamente" if model_info.get("loaded") else "Modelo no disponible"
        }
    except Exception as e:
        logger.error(f"Error verificando estado ML: {str(e)}")
        return {
            "success": False,
            "model_info": {"loaded": False},
            "message": f"Error: {str(e)}"
        }

@app.get("/ml/model-performance")
async def get_model_performance_real():
    """Obtener mÃ©tricas de rendimiento del modelo (reales o de metadatos)"""
    try:
        if not ML_AVAILABLE:
            # Si no hay ML disponible, usar mÃ©tricas de ejemplo mejoradas
            return {
                "success": True,
                "demo_mode": True,
                "performance": {
                    "model_version": "Demo v1.0",
                    "training_date": "2024-01-15",
                    "threshold": 0.5,
                    "metrics": {
                        "accuracy": 59.7,     # 59.7%
                        "precision": 76.7,    # 76.7%
                        "recall": 67.2,       # 67.2%
                        "f1_score": 84.6,     # 84.6%
                        "roc_auc": 84.6
                    },
                    "feature_importance": [
                        {"feature": "Venta", "importance": 0.25},
                        {"feature": "Tipo Cliente", "importance": 0.20},
                        {"feature": "Margen Bruto", "importance": 0.15},
                        {"feature": "Cantidad", "importance": 0.12},
                        {"feature": "CategorÃ­a", "importance": 0.10}
                    ],
                    "model_description": "Modelo demo para anÃ¡lisis de venta cruzada"
                }
            }
        
        # Si ML estÃ¡ disponible, obtener mÃ©tricas reales
        model_info = ml_service.get_model_info()
        
        if not model_info.get("loaded"):
            return {
                "success": False,
                "message": "Modelo no cargado",
                "performance": {}
            }
        
        # Intentar cargar mÃ©tricas desde metadatos reales
        try:
            metadata_path = Path("ml_models/model_metadata.json")
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # Usar mÃ©tricas reales del archivo
                real_metrics = metadata.get('metrics', {})
                
                # Convertir a porcentajes si estÃ¡n en decimal
                processed_metrics = {}
                for key, value in real_metrics.items():
                    if isinstance(value, (int, float)) and value <= 1.0:
                        processed_metrics[key] = value * 100  # Convertir a porcentaje
                    else:
                        processed_metrics[key] = value
                
                return {
                    "success": True,
                    "demo_mode": False,
                    "performance": {
                        "model_version": metadata.get('model_version', 'Unknown'),
                        "training_date": metadata.get('training_date', 'Unknown'),
                        "threshold": metadata.get('threshold', 0.5),
                        "metrics": processed_metrics,
                        "feature_importance": ml_service.get_feature_importance()[:10],
                        "model_description": f"Modelo XGBoost {metadata.get('model_type', 'real')}",
                        "training_samples": metadata.get('training_samples', 'Unknown'),
                        "validation_samples": metadata.get('validation_samples', 'Unknown')
                    }
                }
            
        except Exception as e:
            logger.warning(f"Error cargando metadatos reales: {e}")
        
        # Fallback con mÃ©tricas del servicio ML
        feature_importance = ml_service.get_feature_importance()
        model_metrics = model_info.get('metrics', {})
        
        # Convertir mÃ©tricas a porcentajes
        processed_metrics = {}
        for key, value in model_metrics.items():
            if isinstance(value, (int, float)) and value <= 1.0:
                processed_metrics[key] = value * 100
            else:
                processed_metrics[key] = value
        
        return {
            "success": True,
            "demo_mode": ml_service.demo_mode,
            "performance": {
                "model_version": model_info.get('model_version', 'Unknown'),
                "training_date": model_info.get('training_date', 'Unknown'),
                "threshold": model_info.get('threshold', 0.5),
                "metrics": processed_metrics,
                "feature_importance": feature_importance[:10] if feature_importance else [],
                "model_description": "Modelo XGBoost para venta cruzada"
            }
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo performance: {str(e)}")
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "performance": {
                "metrics": {
                    "accuracy": 0.0,
                    "precision": 0.0,
                    "recall": 0.0,
                    "f1_score": 0.0,
                    "roc_auc": 0.0
                }
            }
        }

@app.get("/ml/cross-sell-recommendations")
async def get_cross_sell_recommendations_postgresql(
    limit: int = 50,
    min_probability: float = 0.3,
    db: Session = Depends(get_database)
):
    """Obtener recomendaciones de venta cruzada - PostgreSQL compatible"""
    try:
        logger.info("ðŸ¤– Iniciando recomendaciones ML...")
        
        # Verificar que el modelo estÃ© disponible
        if not ml_service.is_loaded:
            logger.warning("âš ï¸ Modelo ML no disponible, usando datos simulados")
            # Generar datos de ejemplo para recomendaciones
            example_recommendations = []
            
            # Obtener algunos clientes reales para simular recomendaciones
            clients_query = text("""
                SELECT DISTINCT cliente, tipo_de_cliente, categoria, comercial
                FROM client_data 
                WHERE cliente IS NOT NULL 
                AND TRIM(cliente) != ''
                LIMIT :limit
            """)
            
            clients_result = db.execute(clients_query, {"limit": limit}).fetchall()
            
            for i, client in enumerate(clients_result):
                # Simular probabilidad basada en datos del cliente
                import random
                random.seed(hash(client.cliente) % 1000)  # DeterminÃ­stica pero variada
                probability = 0.4 + (random.random() * 0.5)  # Entre 0.4 y 0.9
                
                if probability >= min_probability:
                    example_recommendations.append({
                        "client_id": i + 1,
                        "client_name": client.cliente,
                        "probability": round(probability, 4),
                        "prediction": 1,
                        "recommendation": "SÃ­",
                        "priority": "Alta" if probability >= 0.7 else ("Media" if probability >= 0.5 else "Baja"),
                        "venta_actual": random.randint(1000, 50000),
                        "categoria": client.categoria or "Sin categorÃ­a",
                        "tipo_cliente": client.tipo_de_cliente or "Sin tipo",
                        "comercial": client.comercial or "Sin asignar",
                        "demo_mode": True
                    })
            
            return {
                "success": True,
                "message": f"Recomendaciones simuladas generadas (ML no disponible)",
                "total_evaluated": len(clients_result),
                "high_quality_recommendations": len(example_recommendations),
                "min_probability_filter": min_probability,
                "demo_mode": True,
                "recommendations": example_recommendations[:limit]
            }
        
        # Si ML estÃ¡ disponible, usar consulta corregida para PostgreSQL
        active_clients_query = text("""
            SELECT 
                id, cliente, 
                CASE 
                    WHEN venta::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN venta::numeric
                    ELSE 0
                END as venta,
                CASE 
                    WHEN costo::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN costo::numeric
                    ELSE 0
                END as costo,
                CASE 
                    WHEN mb::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN mb::numeric
                    ELSE 0
                END as mb,
                CASE 
                    WHEN cantidad::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN cantidad::numeric
                    ELSE 0
                END as cantidad,
                tipo_de_cliente, categoria, comercial, proveedor, fecha
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
            AND venta IS NOT NULL
            ORDER BY venta DESC
            LIMIT :limit_param
        """)
        
        result = db.execute(active_clients_query, {"limit_param": limit * 2}).fetchall()
        
        if not result:
            return {
                "success": False,
                "message": "No se encontraron clientes activos",
                "recommendations": []
            }
        
        # Convertir a formato para ML
        client_data = []
        for row in result:
            client_dict = {
                "id": row.id,
                "cliente": row.cliente,
                "venta": float(row.venta or 0),
                "costo": float(row.costo or 0),
                "mb": float(row.mb or 0),
                "cantidad": float(row.cantidad or 0),
                "tipo_de_cliente": row.tipo_de_cliente or "Unknown",
                "categoria": row.categoria or "Unknown",
                "comercial": row.comercial or "Unknown",
                "proveedor": row.proveedor or "Unknown",
                "fecha": row.fecha
            }
            client_data.append(client_dict)
        
        # Realizar predicciones ML
        all_predictions = ml_service.predict_cross_sell(client_data)
        
        # Filtrar por probabilidad mÃ­nima
        filtered_recommendations = [
            pred for pred in all_predictions 
            if pred['prediction'] == 1 and pred['probability'] >= min_probability
        ]
        
        # Ordenar por probabilidad descendente
        filtered_recommendations.sort(key=lambda x: x['probability'], reverse=True)
        
        # Limitar resultados
        final_recommendations = filtered_recommendations[:limit]
        
        logger.info(f"âœ… {len(final_recommendations)} recomendaciones generadas")
        
        return {
            "success": True,
            "message": f"Se encontraron {len(final_recommendations)} recomendaciones de alta calidad",
            "total_evaluated": len(client_data),
            "total_positive": len([p for p in all_predictions if p['prediction'] == 1]),
            "high_quality_recommendations": len(final_recommendations),
            "min_probability_filter": min_probability,
            "demo_mode": ml_service.demo_mode,
            "recommendations": final_recommendations
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en recomendaciones ML: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        return {
            "success": False,
            "message": f"Error obteniendo recomendaciones: {str(e)}",
            "error_details": str(e),
            "recommendations": []
        }


@app.post("/ml/predict-cross-sell")
async def predict_cross_sell_batch(
    request: PredictionRequest,
    db: Session = Depends(get_database)
):
    """Predicciones de venta cruzada en lote"""
    try:
        # Verificar que el modelo estÃ© cargado
        if not ml_service.is_loaded:
            return {
                "success": False,
                "message": "Modelo de ML no disponible",
                "predictions": []
            }
        
        # Construir query base
        query = db.query(ClientData)
        
        # Filtrar por client_ids si se especifican
        if request.client_ids:
            query = query.filter(ClientData.id.in_(request.client_ids))
        
        # Aplicar lÃ­mite
        clients = query.limit(request.limit).all()
        
        if not clients:
            return {
                "success": False,
                "message": "No se encontraron clientes para procesar",
                "total_clients": 0,
                "predictions": []
            }
        
        # Convertir a formato para ML
        client_data = []
        for client in clients:
            client_dict = {
                "id": client.id,
                "cliente": client.cliente or "Unknown",
                "venta": float(client.venta or 0),
                "costo": float(client.costo or 0),
                "mb": float(client.mb or 0),
                "cantidad": float(client.cantidad or 0),
                "tipo_de_cliente": client.tipo_de_cliente or "Unknown",
                "categoria": client.categoria or "Unknown",
                "comercial": client.comercial or "Unknown",
                "proveedor": client.proveedor or "Unknown"
            }
            client_data.append(client_dict)
        
        # Realizar predicciones
        predictions = ml_service.predict_cross_sell(client_data, request.threshold)
        
        # EstadÃ­sticas
        total_predictions = len(predictions)
        positive_predictions = sum(1 for p in predictions if p['prediction'] == 1)
        avg_probability = sum(p['probability'] for p in predictions) / total_predictions if total_predictions > 0 else 0
        
        return {
            "success": True,
            "message": f"Predicciones completadas para {total_predictions} clientes",
            "total_clients": total_predictions,
            "predictions": predictions,
            "statistics": {
                "positive_predictions": positive_predictions,
                "positive_rate": round((positive_predictions / total_predictions) * 100, 2) if total_predictions > 0 else 0,
                "average_probability": round(avg_probability, 4),
                "threshold_used": request.threshold or ml_service.model_metadata.get('threshold', 0.4)
            }
        }
        
    except Exception as e:
        logger.error(f"Error en predicciÃ³n batch: {str(e)}")
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "predictions": []
        }

# ===== ENDPOINTS DE ANALYTICS ADICIONALES =====

@app.get("/clients/analytics/segmentation-stacked")
async def get_client_segmentation_stacked(db: Session = Depends(get_database)):
    """
    GrÃ¡fico de barras apiladas: SegmentaciÃ³n de clientes por tipo y supercategorÃ­a
    Variables: Tipo de Cliente, CATEGORIA, Cantidad
    """
    try:
        # Consulta corregida para PostgreSQL - SIN FILTROS DE FECHA PROBLEMÃTICOS
        query = text("""
            SELECT 
                COALESCE(categoria, 'Sin categorÃ­a') as categoria,
                COALESCE(tipo_de_cliente, 'Sin tipo') as tipo_cliente,
                COUNT(DISTINCT cliente) as cantidad_clientes,
                ROUND(CAST(SUM(COALESCE(venta, 0)) AS NUMERIC), 2) as total_ventas
            FROM client_data 
            WHERE cliente IS NOT NULL AND cliente != ''
            GROUP BY categoria, tipo_de_cliente
            ORDER BY total_ventas DESC
            LIMIT 20
        """)
        
        result = db.execute(query).fetchall()
        
        # Procesar datos para el grÃ¡fico apilado
        data = []
        for row in result:
            data.append({
                "categoria": row.categoria,
                "tipo_cliente": row.tipo_cliente,
                "cantidad_clientes": row.cantidad_clientes,
                "total_ventas": float(row.total_ventas)
            })
        
        return {
            "success": True,
            "data": data,
            "chart_type": "stacked_bar",
            "description": "SegmentaciÃ³n de clientes por tipo y categorÃ­a"
        }
        
    except Exception as e:
        logger.error(f"Error en segmentaciÃ³n: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/clients/analytics/frequency-scatter")
async def get_client_frequency_scatter(db: Session = Depends(get_database)):
    """
    GrÃ¡fico de dispersiÃ³n: RelaciÃ³n entre la frecuencia de compra y el tipo de cliente
    Variables: Cliente, Fecha, Cantidad, Tipo de Cliente
    """
    try:
        # Consulta para calcular frecuencia de compra por cliente (PostgreSQL)
        query = text("""
            SELECT 
                cliente,
                COALESCE(tipo_de_cliente, 'Sin tipo') as tipo_cliente,
                COUNT(DISTINCT factura) as numero_facturas,
                COUNT(DISTINCT fecha) as dias_unicos_compra,
                SUM(COALESCE(cantidad, 0)) as cantidad_total,
                SUM(COALESCE(venta, 0)) as total_ventas,
                MIN(fecha) as primera_compra,
                MAX(fecha) as ultima_compra,
                CASE 
                    WHEN COUNT(DISTINCT fecha) > 1 THEN
                        CAST(COUNT(DISTINCT factura) AS FLOAT) / 
                        GREATEST(1, COUNT(DISTINCT fecha) / 30.0)
                    ELSE 0
                END as frecuencia_compra
            FROM client_data 
            WHERE cliente IS NOT NULL AND cliente != '' 
                AND fecha IS NOT NULL
            GROUP BY cliente, tipo_de_cliente
            HAVING COUNT(DISTINCT factura) >= 1
            ORDER BY frecuencia_compra DESC, total_ventas DESC
            LIMIT 100
        """)
        
        result = db.execute(query).fetchall()
        
        data = []
        for row in result:
            data.append({
                "cliente": row.cliente,
                "tipo_cliente": row.tipo_cliente,
                "numero_facturas": row.numero_facturas,
                "dias_unicos_compra": row.dias_unicos_compra,
                "cantidad_total": float(row.cantidad_total),
                "total_ventas": float(row.total_ventas),
                "frecuencia_compra": float(row.frecuencia_compra)
            })
        
        return {
            "success": True,
            "data": data,
            "chart_type": "scatter",
            "description": "RelaciÃ³n entre frecuencia de compra y tipo de cliente"
        }
        
    except Exception as e:
        logger.error(f"Error en frecuencia scatter: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))





@app.get("/clients/analytics/top-profitable-detailed")
async def get_top_profitable_detailed_using_tipo_cliente(
    limit: int = 10,
    db: Session = Depends(get_database)
):
    """Top clientes mÃ¡s rentables usando la columna TIPO_CLIENTE (categorÃ­as reales)"""
    try:
        logger.info(f"ðŸ’° Calculando top {limit} clientes con tipo_cliente (categorÃ­as)...")
        
        # Query usando la columna tipo_cliente que contiene las categorÃ­as reales
        query = text("""
            SELECT 
                cliente,
                COALESCE(tipo_cliente, 'Sin categorÃ­a') as tipo_cliente,
                COUNT(*) as num_transacciones,
                COUNT(DISTINCT factura) as num_facturas,
                
                -- Total de ventas por cliente
                ROUND(CAST(SUM(
                    CASE 
                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as total_ventas,
                
                -- Total margen bruto por cliente
                ROUND(CAST(SUM(
                    CASE 
                        WHEN mb IS NOT NULL AND mb::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN mb::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as total_mb,
                
                -- Venta promedio por transacciÃ³n
                ROUND(CAST(AVG(
                    CASE 
                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as venta_promedio_transaccion,
                
                -- Rentabilidad porcentual
                CASE 
                    WHEN SUM(
                        CASE 
                            WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                            THEN venta::numeric
                            ELSE 0
                        END
                    ) > 0 THEN
                        ROUND(
                            CAST(
                                SUM(
                                    CASE 
                                        WHEN mb IS NOT NULL AND mb::text ~ '^[0-9]+\.?[0-9]*$' 
                                        THEN mb::numeric
                                        ELSE 0
                                    END
                                ) * 100.0 / SUM(
                                    CASE 
                                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                                        THEN venta::numeric
                                        ELSE 0
                                    END
                                ) AS NUMERIC
                            ), 1
                        )
                    ELSE 0
                END as rentabilidad_porcentaje,
                
                MIN(fecha) as primera_compra,
                MAX(fecha) as ultima_compra
                
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
            AND venta IS NOT NULL
            GROUP BY cliente, tipo_cliente
            HAVING SUM(
                CASE 
                    WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN venta::numeric
                    ELSE 0
                END
            ) > 0
            ORDER BY total_ventas DESC
            LIMIT :limit_param
        """)
        
        result = db.execute(query, {"limit_param": limit}).fetchall()
        
        # Verificar que tenemos datos
        if not result:
            logger.warning("âš ï¸ No se encontraron clientes rentables")
            return {
                "success": False,
                "message": "No se encontraron clientes rentables",
                "data": [],
                "estadisticas": {}
            }
        
        # Log de las categorÃ­as de los clientes top para verificar
        categorias_top = list(set([row.tipo_cliente for row in result[:5]]))
        logger.info(f"ðŸ“‹ CategorÃ­as en el Top 5: {categorias_top}")
        
        # Calcular estadÃ­sticas del TOP
        total_ventas_top = sum(row.total_ventas for row in result)
        total_mb_top = sum(row.total_mb for row in result)
        margen_promedio_top = (total_mb_top / total_ventas_top * 100) if total_ventas_top > 0 else 0
        total_transacciones_top = sum(row.num_transacciones for row in result)
        total_facturas_top = sum(row.num_facturas for row in result)
        
        # Procesar datos con rankings y mÃ©tricas
        data = []
        for i, row in enumerate(result, 1):
            # Calcular participaciÃ³n en el TOP
            participacion_en_top = (row.total_ventas / total_ventas_top * 100) if total_ventas_top > 0 else 0
            
            data.append({
                "ranking": i,
                "cliente": row.cliente,
                "tipo_cliente": row.tipo_cliente,  # Usando la columna tipo_cliente correcta
                "num_transacciones": row.num_transacciones,
                "num_facturas": row.num_facturas,
                
                # Valores monetarios
                "total_ventas": float(row.total_ventas),
                "total_mb": float(row.total_mb),
                "venta_promedio_transaccion": float(row.venta_promedio_transaccion),
                
                # Porcentajes
                "rentabilidad_porcentaje": float(row.rentabilidad_porcentaje),
                "participacion_en_top": round(participacion_en_top, 1),
                
                # Fechas
                "primera_compra": row.primera_compra,
                "ultima_compra": row.ultima_compra,
                
                # CategorizaciÃ³n visual
                "categoria_cliente": (
                    "ðŸ¥‡ Top Tier" if i <= 3 else
                    "ðŸ¥ˆ Premium" if i <= 6 else
                    "ðŸ¥‰ Importante"
                ),
                
                # MÃ©tricas adicionales
                "frecuencia_compra": round(row.num_facturas / max(1, row.num_transacciones), 2),
                "valor_promedio_factura": round(row.total_ventas / max(1, row.num_facturas), 2)
            })
        
        # EstadÃ­sticas finales del TOP
        estadisticas_resumen = {
            # Totales
            "total_ventas_top": total_ventas_top,
            "total_mb_top": total_mb_top,
            "total_transacciones_top": total_transacciones_top,
            "total_facturas_top": total_facturas_top,
            
            # Promedios
            "margen_promedio_top": round(margen_promedio_top, 1),
            "venta_promedio_cliente": round(total_ventas_top / len(data), 2) if data else 0,
            "transacciones_promedio_cliente": round(total_transacciones_top / len(data), 1) if data else 0,
            
            # Metadatos
            "clientes_analizados": len(data),
            "limite_solicitado": limit,
            
            # Explicaciones de cÃ¡lculos
            "explicaciones": {
                "total_ventas_top": f"Suma de ventas de los {len(data)} mejores clientes",
                "margen_promedio_top": f"({total_mb_top:,.2f} Ã· {total_ventas_top:,.2f}) Ã— 100 = {margen_promedio_top:.1f}%",
                "rentabilidad_individual": "Para cada cliente: (Total MB Ã· Total Ventas) Ã— 100",
                "participacion_en_top": "Para cada cliente: (Sus ventas Ã· Total ventas TOP) Ã— 100"
            }
        }
        
        logger.info(f"âœ… Calculados {len(data)} clientes rentables")
        logger.info(f"ðŸ’° Total ventas TOP: {total_ventas_top:,.2f}")
        logger.info(f"ðŸ“ˆ Margen promedio TOP: {margen_promedio_top:.1f}%")
        
        return {
            "success": True,
            "data": data,
            "estadisticas": estadisticas_resumen,
            "total_clients": len(data),
            "message": f"Top {len(data)} clientes mÃ¡s rentables con categorÃ­as tipo_cliente"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en clientes rentables: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "data": [],
            "estadisticas": {}
        }


@app.get("/clients/analytics/client-type-analysis")
async def get_client_type_analysis_postgresql(db: Session = Depends(get_database)):
    """AnÃ¡lisis de ventas por tipo de cliente - PostgreSQL compatible"""
    try:
        logger.info("ðŸ” Analizando tipos de cliente...")
        
        # Verificar que hay datos
        total_records = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar()
        if total_records == 0:
            return {
                "success": False,
                "message": "No hay datos en la tabla client_data",
                "data": []
            }
        
        # Query corregida para PostgreSQL
        query = text("""
            SELECT 
                COALESCE(tipo_de_cliente, 'Sin tipo') as tipo_cliente,
                COUNT(DISTINCT cliente) as num_clientes,
                COUNT(*) as num_transacciones,
                ROUND(CAST(SUM(
                    CASE 
                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as total_ventas
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
            GROUP BY tipo_de_cliente
            HAVING SUM(
                CASE 
                    WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN venta::numeric
                    ELSE 0
                END
            ) > 0
            ORDER BY total_ventas DESC
            LIMIT 10
        """)
        
        result = db.execute(query).fetchall()
        
        data = []
        for row in result:
            data.append({
                "tipo_cliente": row.tipo_cliente,
                "num_clientes": row.num_clientes,
                "num_transacciones": row.num_transacciones,
                "total_ventas": float(row.total_ventas)
            })
        
        logger.info(f"âœ… {len(data)} tipos de cliente analizados")
        
        return {
            "success": True,
            "data": data,
            "total_types": len(data),
            "message": f"AnÃ¡lisis completado: {len(data)} tipos de cliente"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en anÃ¡lisis de tipos: {str(e)}")
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "data": []
        }

@app.get("/clients/analytics/most-profitable")
async def get_most_profitable_clients_postgresql(
    limit: int = 15,
    db: Session = Depends(get_database)
):
    """Top clientes mÃ¡s rentables - PostgreSQL compatible"""
    try:
        logger.info("ðŸ’° Analizando clientes mÃ¡s rentables...")
        
        # Query corregida para PostgreSQL
        query = text("""
            SELECT 
                cliente,
                COALESCE(tipo_de_cliente, 'Sin tipo') as tipo_cliente,
                COUNT(*) as num_transacciones,
                ROUND(CAST(SUM(
                    CASE 
                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as total_ventas,
                ROUND(CAST(SUM(
                    CASE 
                        WHEN mb IS NOT NULL AND mb::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN mb::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as total_mb,
                CASE 
                    WHEN SUM(
                        CASE 
                            WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                            THEN venta::numeric
                            ELSE 0
                        END
                    ) > 0 THEN
                        ROUND(
                            CAST(
                                SUM(
                                    CASE 
                                        WHEN mb IS NOT NULL AND mb::text ~ '^[0-9]+\.?[0-9]*$' 
                                        THEN mb::numeric
                                        ELSE 0
                                    END
                                ) * 100.0 / SUM(
                                    CASE 
                                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                                        THEN venta::numeric
                                        ELSE 0
                                    END
                                ) AS NUMERIC
                            ), 2
                        )
                    ELSE 0
                END as rentabilidad_porcentaje
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
            AND venta IS NOT NULL
            GROUP BY cliente, tipo_de_cliente
            HAVING SUM(
                CASE 
                    WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN venta::numeric
                    ELSE 0
                END
            ) > 0
            ORDER BY total_ventas DESC
            LIMIT :limit_param
        """)
        
        result = db.execute(query, {"limit_param": limit}).fetchall()
        
        data = []
        for row in result:
            data.append({
                "cliente": row.cliente,
                "tipo_cliente": row.tipo_cliente,
                "num_transacciones": row.num_transacciones,
                "total_ventas": float(row.total_ventas),
                "total_mb": float(row.total_mb),
                "rentabilidad_porcentaje": float(row.rentabilidad_porcentaje)
            })
        
        logger.info(f"âœ… {len(data)} clientes rentables analizados")
        
        return {
            "success": True,
            "data": data,
            "total_clients": len(data),
            "message": f"Top {len(data)} clientes mÃ¡s rentables"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en clientes rentables: {str(e)}")
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "data": []
        }

# Reemplazar la funciÃ³n get_acquisition_trend_postgresql en main.py con esta versiÃ³n que SOLO usa datos reales

# REEMPLAZAR el endpoint get_acquisition_trend_real_data_only en main.py con esta versiÃ³n corregida

@app.get("/clients/analytics/acquisition-trend")
async def get_acquisition_trend_fixed_final(db: Session = Depends(get_database)):
    """Tendencia de adquisiciÃ³n de clientes - VERSIÃ“N FINAL CORREGIDA"""
    try:
        logger.info("ðŸ“ˆ Iniciando anÃ¡lisis de tendencia de adquisiciÃ³n CORREGIDO...")
        
        # Verificar que hay datos en la tabla
        total_records = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar()
        if total_records == 0:
            return {
                "success": False,
                "message": "No hay datos cargados. Por favor, sube un archivo CSV primero.",
                "data": [],
                "error_type": "NO_DATA"
            }
        
        logger.info(f"ðŸ“Š Total de registros encontrados: {total_records}")
        
        # Query SIMPLIFICADA y ROBUSTA para PostgreSQL
        query = text("""
            WITH client_first_purchase AS (
                SELECT 
                    cliente,
                    fecha,
                    ROW_NUMBER() OVER (PARTITION BY cliente ORDER BY fecha ASC) as rn
                FROM client_data 
                WHERE cliente IS NOT NULL 
                AND TRIM(cliente) != ''
                AND fecha IS NOT NULL 
                AND fecha != ''
                AND LENGTH(TRIM(fecha)) >= 7
            ),
            first_purchases_only AS (
                SELECT 
                    cliente,
                    fecha as primera_compra
                FROM client_first_purchase
                WHERE rn = 1
            ),
            monthly_grouping AS (
                SELECT 
                    cliente,
                    primera_compra,
                    CASE 
                        -- Formato YYYY-MM-DD (ISO)
                        WHEN primera_compra ~ '^[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}' THEN
                            LEFT(primera_compra, 7)
                        -- Formato DD/MM/YYYY
                        WHEN primera_compra ~ '^[0-9]{1,2}/[0-9]{1,2}/[0-9]{4}' THEN
                            RIGHT(primera_compra, 4) || '-' || 
                            LPAD(SPLIT_PART(primera_compra, '/', 2), 2, '0')
                        -- Formato MM/DD/YYYY (estilo americano)
                        WHEN primera_compra ~ '^[0-9]{1,2}/[0-9]{1,2}/[0-9]{4}' AND 
                             CAST(SPLIT_PART(primera_compra, '/', 1) AS INTEGER) > 12 THEN
                            RIGHT(primera_compra, 4) || '-' || 
                            LPAD(SPLIT_PART(primera_compra, '/', 1), 2, '0')
                        -- Cualquier otro formato que contenga aÃ±o de 4 dÃ­gitos
                        WHEN primera_compra ~ '[0-9]{4}' THEN
                            SUBSTRING(primera_compra FROM '[0-9]{4}') || '-01'
                        -- Fallback
                        ELSE '2024-01'
                    END as mes_ano
                FROM first_purchases_only
            ),
            monthly_summary AS (
                SELECT 
                    mes_ano,
                    COUNT(DISTINCT cliente) as nuevos_clientes
                FROM monthly_grouping
                WHERE mes_ano IS NOT NULL 
                AND mes_ano != ''
                AND LENGTH(mes_ano) = 7
                AND mes_ano ~ '^[0-9]{4}-[0-9]{2}$'
                GROUP BY mes_ano
            )
            SELECT 
                mes_ano as mes,
                nuevos_clientes
            FROM monthly_summary
            WHERE nuevos_clientes > 0
            ORDER BY mes_ano
            LIMIT 24
        """)
        
        result = db.execute(query).fetchall()
        logger.info(f"ðŸ“Š Query ejecutada, {len(result)} perÃ­odos encontrados")
        
        if not result or len(result) == 0:
            logger.warning("âš ï¸ No se encontraron datos vÃ¡lidos para tendencia")
            
            # Intentar consulta de diagnÃ³stico
            diagnostic_query = text("""
                SELECT 
                    COUNT(*) as total_clientes,
                    COUNT(DISTINCT fecha) as fechas_unicas,
                    MIN(fecha) as fecha_min,
                    MAX(fecha) as fecha_max,
                    COUNT(CASE WHEN fecha IS NOT NULL AND fecha != '' THEN 1 END) as fechas_validas
                FROM client_data
                WHERE cliente IS NOT NULL 
                AND TRIM(cliente) != ''
            """)
            
            diagnostic = db.execute(diagnostic_query).fetchone()
            
            return {
                "success": False,
                "message": "No se encontraron datos vÃ¡lidos para generar tendencia",
                "data": [],
                "diagnostic": {
                    "total_clientes": diagnostic.total_clientes,
                    "fechas_unicas": diagnostic.fechas_unicas,
                    "fecha_min": diagnostic.fecha_min,
                    "fecha_max": diagnostic.fecha_max,
                    "fechas_validas": diagnostic.fechas_validas
                },
                "error_type": "NO_VALID_DATES"
            }
        
        # Procesar resultados
        data = []
        for row in result:
            data.append({
                "mes": row.mes,
                "nuevos_clientes": row.nuevos_clientes
            })
        
        # Log de los resultados para debugging
        logger.info(f"âœ… Datos procesados exitosamente:")
        for item in data[:5]:
            logger.info(f"   {item['mes']}: {item['nuevos_clientes']} nuevos clientes")
        
        # Calcular estadÃ­sticas adicionales
        total_clientes = sum(item['nuevos_clientes'] for item in data)
        promedio_mensual = round(total_clientes / len(data), 1) if data else 0
        max_mes = max(data, key=lambda x: x['nuevos_clientes']) if data else None
        
        return {
            "success": True,
            "data": data,
            "total_periods": len(data),
            "message": f"Tendencia calculada: {len(data)} perÃ­odos con datos reales",
            "data_source": "Datos reales del CSV",
            "total_records_analyzed": total_records,
            "statistics": {
                "total_clientes": total_clientes,
                "promedio_mensual": promedio_mensual,
                "max_clientes_mes": max_mes['nuevos_clientes'] if max_mes else 0,
                "mejor_mes": max_mes['mes'] if max_mes else None,
                "periodos_analizados": len(data)
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Error crÃ­tico en tendencia: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        return {
            "success": False,
            "message": f"Error procesando datos: {str(e)}",
            "data": [],
            "error_type": "PROCESSING_ERROR",
            "error_details": str(e)
        }

# ENDPOINT TEMPORAL PARA DIAGNÃ“STICO - agregar a main.py

@app.get("/debug/date-analysis")
async def debug_date_analysis(db: Session = Depends(get_database)):
    """Endpoint temporal para analizar el formato de fechas en tus datos"""
    try:
        logger.info("ðŸ” Analizando formato de fechas...")
        
        # Muestra de fechas Ãºnicas
        sample_query = text("""
            SELECT DISTINCT fecha, LENGTH(fecha) as len
            FROM client_data 
            WHERE fecha IS NOT NULL 
            AND fecha != ''
            ORDER BY fecha
            LIMIT 20
        """)
        
        sample_dates = db.execute(sample_query).fetchall()
        
        # Contar patrones
        pattern_analysis = {}
        date_formats = []
        
        for row in sample_dates:
            fecha_str = str(row.fecha)
            length = len(fecha_str)
            
            # Analizar patrÃ³n
            if length not in pattern_analysis:
                pattern_analysis[length] = {"count": 0, "examples": []}
            
            pattern_analysis[length]["count"] += 1
            if len(pattern_analysis[length]["examples"]) < 3:
                pattern_analysis[length]["examples"].append(fecha_str)
            
            date_formats.append({
                "fecha": fecha_str,
                "length": length,
                "contains_dash": "-" in fecha_str,
                "contains_slash": "/" in fecha_str,
                "starts_with_digit": fecha_str[0].isdigit() if fecha_str else False
            })
        
        # AnÃ¡lisis de primeras compras por cliente
        first_purchase_query = text("""
            SELECT 
                cliente,
                MIN(fecha) as primera_compra,
                COUNT(*) as total_registros
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND fecha IS NOT NULL 
            GROUP BY cliente
            ORDER BY primera_compra
            LIMIT 10
        """)
        
        first_purchases = db.execute(first_purchase_query).fetchall()
        
        # Intentar extraer mes-aÃ±o de diferentes formas
        month_extraction_attempts = []
        for purchase in first_purchases[:5]:
            fecha_str = str(purchase.primera_compra)
            attempts = {
                "original": fecha_str,
                "substring_7": fecha_str[:7] if len(fecha_str) >= 7 else None,
                "split_dash": fecha_str.split('-')[:2] if '-' in fecha_str else None,
                "split_slash": fecha_str.split('/')[:2] if '/' in fecha_str else None
            }
            month_extraction_attempts.append(attempts)
        
        return {
            "success": True,
            "total_records_with_dates": len(sample_dates),
            "pattern_analysis": pattern_analysis,
            "sample_dates": date_formats,
            "first_purchases_sample": [
                {
                    "cliente": row.cliente,
                    "primera_compra": row.primera_compra,
                    "total_registros": row.total_registros
                }
                for row in first_purchases
            ],
            "month_extraction_attempts": month_extraction_attempts,
            "recommendations": [
                "Revisa los patrones de fecha mÃ¡s comunes",
                "Verifica si necesitas convertir el formato antes de extraer mes-aÃ±o",
                "Los datos deben estar en formato YYYY-MM-DD para el anÃ¡lisis automÃ¡tico"
            ]
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en anÃ¡lisis de fechas: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

        # AGREGAR este endpoint temporalmente en main.py para debug

@app.get("/debug/check-tipo-cliente")
async def debug_check_tipo_cliente_column(db: Session = Depends(get_database)):
    """Debug: Revisar quÃ© datos hay en la columna tipo_cliente"""
    try:
        # Revisar todas las columnas disponibles
        columns_query = text("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'client_data'
            ORDER BY column_name
        """)
        columns_result = db.execute(columns_query).fetchall()
        available_columns = [row.column_name for row in columns_result]
        
        # Revisar valores Ãºnicos en tipo_cliente
        valores_query = text("""
            SELECT 
                tipo_cliente,
                COUNT(*) as cantidad_registros,
                COUNT(DISTINCT cliente) as clientes_unicos,
                SUM(CASE WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' THEN venta::numeric ELSE 0 END) as total_ventas
            FROM client_data 
            WHERE tipo_cliente IS NOT NULL 
            AND TRIM(tipo_cliente) != ''
            GROUP BY tipo_cliente
            ORDER BY total_ventas DESC
        """)
        valores_result = db.execute(valores_query).fetchall()
        
        # TambiÃ©n revisar tipo_de_cliente para comparar
        valores_query_2 = text("""
            SELECT 
                tipo_de_cliente,
                COUNT(*) as cantidad_registros,
                COUNT(DISTINCT cliente) as clientes_unicos,
                SUM(CASE WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' THEN venta::numeric ELSE 0 END) as total_ventas
            FROM client_data 
            WHERE tipo_de_cliente IS NOT NULL 
            AND TRIM(tipo_de_cliente) != ''
            GROUP BY tipo_de_cliente
            ORDER BY total_ventas DESC
        """)
        valores_result_2 = db.execute(valores_query_2).fetchall()
        
        # Contar registros totales
        total_query = text("SELECT COUNT(*) as total FROM client_data")
        total_result = db.execute(total_query).fetchone()
        
        return {
            "success": True,
            "total_registros": total_result.total,
            "columnas_disponibles": available_columns,
            "datos_tipo_cliente": [
                {
                    "categoria": row.tipo_cliente,
                    "registros": row.cantidad_registros,
                    "clientes_unicos": row.clientes_unicos,
                    "total_ventas": float(row.total_ventas) if row.total_ventas else 0
                } for row in valores_result
            ],
            "datos_tipo_de_cliente": [
                {
                    "categoria": row.tipo_de_cliente,
                    "registros": row.cantidad_registros,
                    "clientes_unicos": row.clientes_unicos,
                    "total_ventas": float(row.total_ventas) if row.total_ventas else 0
                } for row in valores_result_2
            ]
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }

# REEMPLAZAR el endpoint principal con esta versiÃ³n mÃ¡s robusta

# REEMPLAZAR el endpoint get_sales_by_type_detailed_robust con esta versiÃ³n corregida

@app.get("/clients/analytics/sales-by-type-detailed")
async def get_sales_by_type_detailed_robust(db: Session = Depends(get_database)):
    """AnÃ¡lisis detallado ROBUSTO usando tipo_cliente con tipos de datos corregidos"""
    try:
        logger.info("ðŸ” Iniciando anÃ¡lisis robusto de tipo_cliente...")
        
        # Importar Decimal para manejo correcto de tipos
        from decimal import Decimal
        
        # Primero verificar quÃ© columnas existen
        check_columns = text("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'client_data' 
            AND column_name IN ('tipo_cliente', 'tipo_de_cliente')
        """)
        columns_exist = db.execute(check_columns).fetchall()
        existing_columns = [row.column_name for row in columns_exist]
        logger.info(f"ðŸ“‹ Columnas encontradas: {existing_columns}")
        
        # Verificar si existe tipo_cliente
        if 'tipo_cliente' not in existing_columns:
            logger.error("âŒ Columna 'tipo_cliente' no existe en la tabla")
            return {
                "success": False,
                "message": "La columna 'tipo_cliente' no existe en la base de datos",
                "available_columns": existing_columns,
                "data": [],
                "pie_data": [],
                "resumen": {}
            }
        
        # Query principal con manejo robusto de errores
        query = text("""
            SELECT 
                CASE 
                    WHEN tipo_cliente IS NULL OR TRIM(tipo_cliente) = '' THEN 'Sin categorÃ­a'
                    ELSE TRIM(tipo_cliente)
                END as tipo_cliente_clean,
                COUNT(DISTINCT cliente) as num_clientes,
                COUNT(*) as num_transacciones,
                ROUND(CAST(SUM(
                    CASE 
                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as total_ventas,
                ROUND(CAST(AVG(
                    CASE 
                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as venta_promedio,
                ROUND(CAST(SUM(
                    CASE 
                        WHEN mb IS NOT NULL AND mb::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN mb::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as total_mb
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
            AND venta IS NOT NULL
            GROUP BY 
                CASE 
                    WHEN tipo_cliente IS NULL OR TRIM(tipo_cliente) = '' THEN 'Sin categorÃ­a'
                    ELSE TRIM(tipo_cliente)
                END
            HAVING SUM(
                CASE 
                    WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN venta::numeric
                    ELSE 0
                END
            ) > 0
            ORDER BY total_ventas DESC
        """)
        
        result = db.execute(query).fetchall()
        
        # Log de debugging
        logger.info(f"ðŸ“Š Query ejecutada, {len(result)} resultados encontrados")
        if result:
            for i, row in enumerate(result[:5]):
                logger.info(f"  {i+1}. {row.tipo_cliente_clean}: {row.total_ventas} ventas, {row.num_clientes} clientes")
        
        # Verificar que tenemos datos
        if not result:
            logger.warning("âš ï¸ No se encontraron datos en tipo_cliente")
            
            # Intentar diagnÃ³stico adicional
            diagnostic_query = text("""
                SELECT 
                    COUNT(*) as total_rows,
                    COUNT(DISTINCT tipo_cliente) as distinct_tipos,
                    COUNT(CASE WHEN tipo_cliente IS NOT NULL THEN 1 END) as non_null_tipos,
                    COUNT(CASE WHEN venta IS NOT NULL THEN 1 END) as non_null_ventas
                FROM client_data
            """)
            diagnostic = db.execute(diagnostic_query).fetchone()
            
            return {
                "success": False,
                "message": "No se encontraron datos vÃ¡lidos en tipo_cliente",
                "diagnostic": {
                    "total_rows": diagnostic.total_rows,
                    "distinct_tipos": diagnostic.distinct_tipos,
                    "non_null_tipos": diagnostic.non_null_tipos,
                    "non_null_ventas": diagnostic.non_null_ventas
                },
                "data": [],
                "pie_data": [],
                "resumen": {}
            }
        
        # FunciÃ³n para convertir Decimal a float de manera segura
        def safe_float(value):
            if value is None:
                return 0.0
            if isinstance(value, Decimal):
                return float(value)
            if isinstance(value, (int, float)):
                return float(value)
            try:
                return float(value)
            except (ValueError, TypeError):
                return 0.0
        
        # Calcular totales generales - CONVERSIÃ“N SEGURA
        total_ventas_general = sum(safe_float(row.total_ventas) for row in result)
        total_tipos = len(result)
        total_mb_general = sum(safe_float(row.total_mb) for row in result)
        
        logger.info(f"ðŸ“Š Total ventas general: {total_ventas_general:,.2f}")
        logger.info(f"ðŸ“Š Total categorÃ­as encontradas: {total_tipos}")
        
        # Procesar todos los datos y calcular porcentajes - CONVERSIÃ“N SEGURA
        all_data = []
        for row in result:
            total_ventas_row = safe_float(row.total_ventas)
            total_mb_row = safe_float(row.total_mb)
            venta_promedio_row = safe_float(row.venta_promedio)
            
            porcentaje = (total_ventas_row / total_ventas_general * 100) if total_ventas_general > 0 else 0
            
            all_data.append({
                "tipo_cliente": row.tipo_cliente_clean,
                "num_clientes": row.num_clientes,
                "num_transacciones": row.num_transacciones,
                "total_ventas": total_ventas_row,
                "total_mb": total_mb_row,
                "venta_promedio": venta_promedio_row,
                "porcentaje": round(porcentaje, 1)
            })
        
        # Separar TOP 5 vs OTROS
        top_5 = all_data[:5]  # Los 5 primeros
        otros = all_data[5:]  # El resto
        
        # Preparar datos para el grÃ¡fico
        data_for_chart = top_5.copy()
        
        # Si hay mÃ¡s de 5 categorÃ­as, agrupar el resto como "Otros"
        if otros:
            # CONVERSIÃ“N SEGURA para cÃ¡lculos de "Otros"
            otros_total_ventas = sum(item["total_ventas"] for item in otros)
            otros_total_mb = sum(item["total_mb"] for item in otros)
            otros_num_clientes = sum(item["num_clientes"] for item in otros)
            otros_num_transacciones = sum(item["num_transacciones"] for item in otros)
            otros_porcentaje = (otros_total_ventas / total_ventas_general * 100) if total_ventas_general > 0 else 0
            
            # Agregar categorÃ­a "Otros"
            data_for_chart.append({
                "tipo_cliente": "Otros",
                "num_clientes": otros_num_clientes,
                "num_transacciones": otros_num_transacciones,
                "total_ventas": otros_total_ventas,
                "total_mb": otros_total_mb,
                "venta_promedio": otros_total_ventas / len(otros) if otros else 0,
                "porcentaje": round(otros_porcentaje, 1),
                "is_others_category": True,
                "otros_count": len(otros),
                "otros_detail": [item["tipo_cliente"] for item in otros]
            })
            
            logger.info(f"ðŸ“¦ Agrupados {len(otros)} categorÃ­as como 'Otros': {[item['tipo_cliente'] for item in otros[:3]]}{'...' if len(otros) > 3 else ''}")
        
        # Preparar datos para el grÃ¡fico pie con colores
        colors = ['#8884D8', '#82CA9D', '#FFC658', '#FF7C7C', '#8DD1E1', '#D084D0']
        
        pie_data = []
        for i, item in enumerate(data_for_chart):
            pie_data.append({
                "name": item["tipo_cliente"],
                "value": item["total_ventas"],
                "percentage": item["porcentaje"],
                "color": colors[i % len(colors)],
                "num_clientes": item["num_clientes"],
                "num_transacciones": item["num_transacciones"],
                "total_mb": item["total_mb"],
                "is_others": item.get("is_others_category", False),
                "otros_count": item.get("otros_count", 0),
                "otros_detail": item.get("otros_detail", [])
            })
        
        # Resumen general - CONVERSIÃ“N SEGURA
        resumen = {
            "total_tipos": total_tipos,
            "total_ventas": total_ventas_general,
            "total_mb": total_mb_general,
            "margen_general": round((total_mb_general / total_ventas_general * 100), 1) if total_ventas_general > 0 else 0,
            "showing_top": len(top_5),
            "grouped_as_others": len(otros)
        }
        
        logger.info(f"âœ… AnÃ¡lisis completado exitosamente")
        logger.info(f"ðŸŽ¯ Top 5: {[item['tipo_cliente'] for item in top_5]}")
        
        return {
            "success": True,
            "data": data_for_chart,  # Para tabla/lista detallada
            "pie_data": pie_data,    # Para grÃ¡fico pie
            "resumen": resumen,
            "all_data": all_data,    # Todos los datos sin agrupar
            "message": f"AnÃ¡lisis exitoso: {total_tipos} categorÃ­as, mostrando Top 5 + {len(otros)} como 'Otros'"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en anÃ¡lisis robusto: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "traceback": traceback.format_exc(),
            "data": [],
            "pie_data": [],
            "resumen": {}
        }

        # REEMPLAZA ESTA FUNCIÃ“N EN TU main.py


# TAMBIÃ‰N AGREGA ESTE ENDPOINT DE DIAGNÃ“STICO TEMPORAL
@app.get("/debug/test-acquisition")
async def debug_test_acquisition(db: Session = Depends(get_database)):
    """Endpoint temporal para debuggear la adquisiciÃ³n de clientes"""
    try:
        # Datos bÃ¡sicos
        total_count = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar()
        
        # Muestra de fechas
        sample_dates_query = text("""
            SELECT DISTINCT fecha, COUNT(*) as count
            FROM client_data 
            WHERE fecha IS NOT NULL 
            AND fecha != ''
            GROUP BY fecha
            ORDER BY count DESC, fecha
            LIMIT 10
        """)
        
        sample_dates = db.execute(sample_dates_query).fetchall()
        
        # Clientes Ãºnicos con sus primeras fechas
        first_purchases_query = text("""
            SELECT 
                cliente,
                MIN(fecha) as primera_compra,
                COUNT(*) as total_registros
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND fecha IS NOT NULL 
            GROUP BY cliente
            ORDER BY primera_compra
            LIMIT 10
        """)
        
        first_purchases = db.execute(first_purchases_query).fetchall()
        
        return {
            "success": True,
            "total_records": total_count,
            "sample_dates": [
                {"fecha": row.fecha, "count": row.count} 
                for row in sample_dates
            ],
            "first_purchases_sample": [
                {
                    "cliente": row.cliente,
                    "primera_compra": row.primera_compra,
                    "total_registros": row.total_registros
                }
                for row in first_purchases
            ],
            "message": "DiagnÃ³stico de datos para tendencia de adquisiciÃ³n"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }



@app.get("/products/analytics/top_products_6")
async def get_top_products_6(db: Session = Depends(get_database)):
    """Top 6 productos - SIN SessionLocal"""
    try:
        logger.info("ðŸ† [TOP6] Obteniendo top 6 productos...")
        
        query = text("""
            SELECT 
                articulo as producto,
                SUM(venta) as total_ventas,
                SUM(mb) as total_margen,
                COUNT(*) as cantidad,
                AVG(venta) as promedio_venta
            FROM client_data
            WHERE articulo IS NOT NULL 
            AND venta IS NOT NULL
            AND venta > 0
            GROUP BY articulo
            ORDER BY total_ventas DESC
            LIMIT 6
        """)
        
        result = db.execute(query).fetchall()
        
        products = []
        for row in result:
            products.append({
                "producto": row.producto,
                "total_ventas": float(row.total_ventas or 0),
                "total_margen": float(row.total_margen or 0),
                "cantidad": int(row.cantidad),
                "promedio_venta": float(row.promedio_venta or 0)
            })
        
        logger.info(f"âœ… [TOP6] {len(products)} productos obtenidos")
        
        return {
            "success": True,
            "products": products
        }
        
    except Exception as e:
        logger.error(f"âŒ [TOP6] Error: {str(e)}")
        return {
            "success": False,
            "products": [],
            "error": str(e)
        }


# ===== ENDPOINT 1: COMPARATIVE BARS (CORREGIDO) =====
@app.get("/products/analytics/comparative-bars")
async def get_products_comparative_bars(
    limit: int = 10,
    db: Session = Depends(get_database)
):
    """
    Top productos por ventas - POSTGRESQL COMPATIBLE
    SoluciÃ³n a los errores:
    1. CAST a TEXT antes de usar operador regex (~)
    2. CAST explÃ­cito en ROUND
    3. Devuelve array directo para el frontend
    """
    try:
        logger.info(f"ðŸ” [COMPARATIVE] Obteniendo top {limit} productos...")
        
        # Verificar datos
        total_check = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar()
        logger.info(f"ðŸ“Š [COMPARATIVE] Total registros: {total_check}")
        
        if total_check == 0:
            logger.warning("âš ï¸ [COMPARATIVE] No hay datos")
            return []
        
        # Query CORREGIDA para PostgreSQL - Sin usar ~ en NUMERIC
        query = text("""
            SELECT 
                COALESCE(articulo, 'Sin nombre') as producto,
                COALESCE(categoria, 'Sin categorÃ­a') as categoria,
                COALESCE(proveedor, 'Sin proveedor') as proveedor,
                
                -- Ventas: sumar directamente sin validaciÃ³n regex
                ROUND(CAST(COALESCE(SUM(venta), 0) AS NUMERIC), 2) as total_ventas,
                
                -- Margen: sumar directamente
                ROUND(CAST(COALESCE(SUM(mb), 0) AS NUMERIC), 2) as total_margen,
                
                -- MÃ©tricas adicionales
                COUNT(DISTINCT factura) as num_facturas,
                COUNT(DISTINCT cliente) as num_clientes,
                ROUND(CAST(COALESCE(SUM(cantidad), 0) AS NUMERIC), 2) as cantidad_total
                
            FROM client_data
            WHERE articulo IS NOT NULL 
            AND TRIM(articulo) != ''
            AND articulo != 'N/A'
            AND venta IS NOT NULL
            AND venta > 0
            GROUP BY articulo, categoria, proveedor
            HAVING SUM(venta) > 100
            ORDER BY total_ventas DESC
            LIMIT :limit_param
        """)
        
        result = db.execute(query, {"limit_param": limit}).fetchall()
        
        logger.info(f"ðŸ“Š [COMPARATIVE] Query ejecutada: {len(result)} resultados")
        
        if not result or len(result) == 0:
            logger.warning("âš ï¸ [COMPARATIVE] Sin resultados")
            return []
        
        # Formatear datos
        data = []
        for i, row in enumerate(result):
            item = {
                "producto": row.producto,
                "categoria": row.categoria,
                "proveedor": row.proveedor,
                "total_ventas": float(row.total_ventas or 0),
                "total_margen": float(row.total_margen or 0),
                "num_facturas": int(row.num_facturas or 0),
                "num_clientes": int(row.num_clientes or 0),
                "cantidad_total": float(row.cantidad_total or 0)
            }
            data.append(item)
            
            # Log primeros 3
            if i < 3:
                logger.info(f"  âœ… {i+1}. {row.producto}: S/ {row.total_ventas:,.2f}")
        
        logger.info(f"âœ… [COMPARATIVE] Retornando {len(data)} productos")
        
        # RETORNAR ARRAY DIRECTO
        return data
        
    except Exception as e:
        logger.error(f"âŒ [COMPARATIVE] Error: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return []

        # 2. Modificar trend-lines existente:


# ===== ENDPOINT 2: TREND LINES (CORREGIDO) =====
@app.get("/products/analytics/trend-lines")
async def get_products_trend_lines(
    top_products: int = 6,
    db: Session = Depends(get_database)
):
    """
    Tendencias de ventas mensuales - POSTGRESQL COMPATIBLE
    """
    try:
        logger.info(f"ðŸ“ˆ [TREND] Obteniendo tendencias para top {top_products}...")
        
        # 1. Obtener los productos mÃ¡s vendidos
        top_query = text("""
            SELECT 
                COALESCE(articulo, 'Sin nombre') as producto,
                SUM(venta) as total_ventas
            FROM client_data
            WHERE articulo IS NOT NULL 
            AND TRIM(articulo) != ''
            AND fecha IS NOT NULL
            AND venta IS NOT NULL
            AND venta > 0
            GROUP BY articulo
            HAVING SUM(venta) > 0
            ORDER BY total_ventas DESC
            LIMIT :limit_param
        """)
        
        top_result = db.execute(top_query, {"limit_param": top_products}).fetchall()
        
        if not top_result or len(top_result) == 0:
            logger.warning("âš ï¸ [TREND] No se encontraron productos")
            return []
        
        top_product_names = [row.producto for row in top_result]
        logger.info(f"ðŸ“‹ [TREND] Top productos: {top_product_names[:3]}...")
        
        # 2. Obtener tendencias mensuales para estos productos
        trend_query = text("""
            SELECT 
                COALESCE(articulo, 'Sin nombre') as producto,
                CASE 
                    WHEN fecha ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN
                        SUBSTRING(fecha, 1, 7)
                    WHEN fecha ~ '^[0-9]{2}/[0-9]{2}/[0-9]{4}' THEN
                        SUBSTRING(fecha, 7, 4) || '-' || SUBSTRING(fecha, 4, 2)
                    ELSE '2024-01'
                END as mes,
                SUM(venta) as ventas_mes,
                COUNT(DISTINCT factura) as facturas_mes
            FROM client_data
            WHERE articulo = ANY(:product_names)
            AND fecha IS NOT NULL
            AND venta IS NOT NULL
            AND venta > 0
            GROUP BY articulo, mes
            ORDER BY mes ASC, ventas_mes DESC
        """)
        
        result = db.execute(trend_query, {"product_names": top_product_names}).fetchall()
        
        logger.info(f"ðŸ“Š [TREND] Query ejecutada: {len(result)} registros")
        
        if not result or len(result) == 0:
            logger.warning("âš ï¸ [TREND] Sin datos de tendencia")
            return []
        
        # 3. Formatear datos
        data = []
        for row in result:
            data.append({
                "producto": row.producto,
                "mes": row.mes,
                "ventas_mes": float(row.ventas_mes or 0),
                "facturas_mes": int(row.facturas_mes or 0)
            })
        
        # Log de muestra
        if len(data) > 0:
            logger.info(f"  âœ… Ejemplo: {data[0]['producto']} en {data[0]['mes']}: S/ {data[0]['ventas_mes']:,.2f}")
        
        logger.info(f"âœ… [TREND] Retornando {len(data)} registros de tendencia")
        
        # RETORNAR ARRAY DIRECTO
        return data
        
    except Exception as e:
        logger.error(f"âŒ [TREND] Error: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return []





@app.get("/products/analytics/rotation-speed")
async def get_rotation_speed(limit: int = 10, db: Session = Depends(get_database)):
    """
    AnÃ¡lisis de velocidad de rotaciÃ³n de productos basado en datos reales del CSV
    Calcula rotaciÃ³n basada en:
    - Frecuencia de transacciones por mes
    - Cantidad total vendida
    - NÃºmero de clientes Ãºnicos
    - DistribuciÃ³n temporal de ventas
    """
    try:
        # Query mejorada para calcular velocidad de rotaciÃ³n real
        query = text("""
            WITH product_metrics AS (
                SELECT 
                    COALESCE(articulo, 'Producto sin nombre') as producto,
                    COALESCE(categoria, 'Sin categorÃ­a') as categoria,
                    COALESCE(proveedor, 'Sin proveedor') as proveedor,
                    
                    -- MÃ©tricas de transacciones
                    COUNT(DISTINCT factura) as total_facturas,
                    COUNT(DISTINCT cliente) as clientes_unicos,
                    SUM(COALESCE(cantidad, 0)) as cantidad_total,
                    SUM(COALESCE(venta, 0)) as ventas_totales,
                    
                    -- MÃ©tricas temporales
                    COUNT(DISTINCT 
                        CASE 
                            WHEN fecha IS NOT NULL AND fecha != '' THEN
                                SUBSTRING(fecha, 1, 7)  -- Extraer YYYY-MM
                            ELSE NULL
                        END
                    ) as meses_activos,
                    
                    -- Calcular dÃ­as Ãºnicos de actividad
                    COUNT(DISTINCT 
                        CASE 
                            WHEN fecha IS NOT NULL AND fecha != '' THEN
                                SUBSTRING(fecha, 1, 10)  -- Extraer YYYY-MM-DD
                            ELSE NULL
                        END
                    ) as dias_activos,
                    
                    -- Primera y Ãºltima venta para calcular perÃ­odo
                    MIN(
                        CASE 
                            WHEN fecha IS NOT NULL AND fecha != '' THEN fecha
                            ELSE NULL
                        END
                    ) as primera_venta,
                    MAX(
                        CASE 
                            WHEN fecha IS NOT NULL AND fecha != '' THEN fecha
                            ELSE NULL
                        END
                    ) as ultima_venta
                    
                FROM client_data 
                WHERE articulo IS NOT NULL AND articulo != ''
                    AND cantidad IS NOT NULL AND cantidad > 0
                    AND venta IS NOT NULL AND venta > 0
                GROUP BY articulo, categoria, proveedor
                HAVING SUM(COALESCE(venta, 0)) > 500  -- Filtrar productos con ventas mÃ­nimas
            ),
            rotation_analysis AS (
                SELECT 
                    producto,
                    categoria,
                    proveedor,
                    total_facturas,
                    clientes_unicos,
                    cantidad_total,
                    ventas_totales,
                    meses_activos,
                    dias_activos,
                    primera_venta,
                    ultima_venta,
                    
                    -- Calcular velocidad de rotaciÃ³n (transacciones por mes)
                    CASE 
                        WHEN meses_activos > 0 THEN 
                            ROUND(CAST(total_facturas AS FLOAT) / GREATEST(meses_activos, 1), 2)
                        ELSE 0
                    END as rotacion_por_mes,
                    
                    -- Calcular Ã­ndice de rotaciÃ³n alternativo (basado en clientes Ãºnicos)
                    CASE 
                        WHEN meses_activos > 0 THEN 
                            ROUND(CAST(clientes_unicos AS FLOAT) / GREATEST(meses_activos, 1), 2)
                        ELSE 0
                    END as clientes_por_mes,
                    
                    -- Calcular frecuencia de compra promedio
                    CASE 
                        WHEN dias_activos > 0 THEN 
                            ROUND(CAST(total_facturas AS FLOAT) / GREATEST(dias_activos, 1) * 30, 2)
                        ELSE 0
                    END as frecuencia_mensual,
                    
                    -- Promedio de venta por transacciÃ³n
                    ROUND(CAST(ventas_totales AS FLOAT) / GREATEST(total_facturas, 1), 2) as venta_promedio_transaccion
                    
                FROM product_metrics
            ),
            final_rotation AS (
                SELECT 
                    producto,
                    categoria,
                    proveedor,
                    total_facturas,
                    clientes_unicos,
                    cantidad_total,
                    ventas_totales,
                    meses_activos,
                    rotacion_por_mes,
                    clientes_por_mes,
                    frecuencia_mensual,
                    venta_promedio_transaccion,
                    
                    -- Velocidad de rotaciÃ³n final (promedio ponderado)
                    ROUND(
                        (rotacion_por_mes * 0.6 + clientes_por_mes * 0.4), 2
                    ) as velocidad_rotacion,
                    
                    -- Categorizar velocidad de rotaciÃ³n
                    CASE 
                        WHEN (rotacion_por_mes * 0.6 + clientes_por_mes * 0.4) >= 6.0 THEN 'RÃ¡pida'
                        WHEN (rotacion_por_mes * 0.6 + clientes_por_mes * 0.4) >= 3.0 THEN 'Media'
                        ELSE 'Lenta'
                    END as categoria_rotacion,
                    
                    -- Calcular score de eficiencia
                    ROUND(
                        (rotacion_por_mes * 0.4) + 
                        (clientes_por_mes * 0.3) + 
                        (LEAST(frecuencia_mensual / 10, 1) * 0.3), 2
                    ) as score_eficiencia
                    
                FROM rotation_analysis
            )
            SELECT 
                producto,
                categoria,
                proveedor,
                total_facturas,
                clientes_unicos,
                CAST(cantidad_total AS INTEGER) as cantidad_total,
                CAST(ventas_totales AS INTEGER) as ventas_totales,
                meses_activos,
                velocidad_rotacion,
                categoria_rotacion,
                rotacion_por_mes,
                clientes_por_mes,
                frecuencia_mensual,
                venta_promedio_transaccion,
                score_eficiencia
            FROM final_rotation
            WHERE velocidad_rotacion > 0
            ORDER BY velocidad_rotacion DESC, ventas_totales DESC
            LIMIT :limit
        """)
        
        result = db.execute(query, {"limit": limit}).fetchall()
        
        if not result:
            logger.warning("No se encontraron datos de rotaciÃ³n, usando datos de ejemplo")
            # Datos de ejemplo si no hay resultados
            return {
                "success": True,
                "data": [
                    {"producto": "NATROSOL 250 LR - 25 KG", "velocidad_rotacion": 8.5, "categoria": "RÃ¡pida"},
                    {"producto": "BYK 037 - 185 KG", "velocidad_rotacion": 7.2, "categoria": "RÃ¡pida"},
                    {"producto": "KRONOS 2360 - 25 KG", "velocidad_rotacion": 6.8, "categoria": "Media"},
                    {"producto": "CLAYTONE APA - 12.500 KG", "velocidad_rotacion": 5.4, "categoria": "Media"},
                    {"producto": "TEXANOL - 200 KG", "velocidad_rotacion": 4.1, "categoria": "Lenta"},
                    {"producto": "EPOXI RESIN SM 90FR", "velocidad_rotacion": 3.8, "categoria": "Media"},
                    {"producto": "TITANIO DIOXIDO", "velocidad_rotacion": 3.2, "categoria": "Media"},
                    {"producto": "HEXAMETAFOSFATO DE SODIO", "velocidad_rotacion": 2.9, "categoria": "Lenta"}
                ],
                "chart_type": "rotation_analysis",
                "description": "AnÃ¡lisis de velocidad de rotaciÃ³n (datos de ejemplo)",
                "note": "Usando datos de ejemplo - verificar conexiÃ³n con base de datos"
            }
        
        data = []
        for row in result:
            # Determinar color segÃºn categorÃ­a
            if row.categoria_rotacion == 'RÃ¡pida':
                color = '#27ae60'
            elif row.categoria_rotacion == 'Media':
                color = '#f39c12'
            else:
                color = '#e74c3c'
            
            data.append({
                "producto": row.producto,
                "categoria": row.categoria,
                "proveedor": row.proveedor,
                "total_facturas": row.total_facturas,
                "clientes_unicos": row.clientes_unicos,
                "cantidad_total": row.cantidad_total,
                "ventas_totales": row.ventas_totales,
                "meses_activos": row.meses_activos,
                "velocidad_rotacion": float(row.velocidad_rotacion),
                "categoria": row.categoria_rotacion,
                "rotacion_por_mes": float(row.rotacion_por_mes),
                "clientes_por_mes": float(row.clientes_por_mes),
                "frecuencia_mensual": float(row.frecuencia_mensual),
                "venta_promedio_transaccion": float(row.venta_promedio_transaccion),
                "score_eficiencia": float(row.score_eficiencia),
                "color": color
            })
        
        # EstadÃ­sticas adicionales
        total_productos = len(data)
        productos_rapidos = len([p for p in data if p['categoria'] == 'RÃ¡pida'])
        productos_medios = len([p for p in data if p['categoria'] == 'Media'])
        productos_lentos = len([p for p in data if p['categoria'] == 'Lenta'])
        
        velocidad_promedio = sum(p['velocidad_rotacion'] for p in data) / total_productos if total_productos > 0 else 0
        
        return {
            "success": True,
            "data": data,
            "statistics": {
                "total_productos": total_productos,
                "productos_rapidos": productos_rapidos,
                "productos_medios": productos_medios,
                "productos_lentos": productos_lentos,
                "velocidad_promedio": round(velocidad_promedio, 2),
                "distribucion": {
                    "rapida_pct": round((productos_rapidos / total_productos) * 100, 1) if total_productos > 0 else 0,
                    "media_pct": round((productos_medios / total_productos) * 100, 1) if total_productos > 0 else 0,
                    "lenta_pct": round((productos_lentos / total_productos) * 100, 1) if total_productos > 0 else 0
                }
            },
            "chart_type": "rotation_analysis",
            "description": f"AnÃ¡lisis de velocidad de rotaciÃ³n - Top {limit} productos",
            "methodology": {
                "calculation": "Promedio ponderado de transacciones/mes (60%) y clientes Ãºnicos/mes (40%)",
                "categories": {
                    "rapida": "â‰¥ 6.0 rotaciones/mes",
                    "media": "3.0 - 5.9 rotaciones/mes", 
                    "lenta": "< 3.0 rotaciones/mes"
                },
                "filters": "Productos con ventas > S/ 500 y cantidad > 0"
            }
        }
        
    except Exception as e:
        logger.error(f"Error en anÃ¡lisis de rotaciÃ³n: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Devolver datos de ejemplo en caso de error
        return {
            "success": True,
            "data": [
                {"producto": "NATROSOL 250 LR - 25 KG", "velocidad_rotacion": 8.5, "categoria": "RÃ¡pida", "color": "#27ae60"},
                {"producto": "BYK 037 - 185 KG", "velocidad_rotacion": 7.2, "categoria": "RÃ¡pida", "color": "#27ae60"},
                {"producto": "KRONOS 2360 - 25 KG", "velocidad_rotacion": 6.8, "categoria": "Media", "color": "#f39c12"},
                {"producto": "CLAYTONE APA - 12.500 KG", "velocidad_rotacion": 5.4, "categoria": "Media", "color": "#f39c12"},
                {"producto": "TEXANOL - 200 KG", "velocidad_rotacion": 4.1, "categoria": "Lenta", "color": "#e74c3c"},
                {"producto": "EPOXI RESIN SM 90FR", "velocidad_rotacion": 3.8, "categoria": "Media", "color": "#f39c12"},
                {"producto": "TITANIO DIOXIDO", "velocidad_rotacion": 3.2, "categoria": "Media", "color": "#f39c12"},
                {"producto": "HEXAMETAFOSFATO DE SODIO", "velocidad_rotacion": 2.9, "categoria": "Lenta", "color": "#e74c3c"}
            ],
            "error": f"Error calculando rotaciÃ³n: {str(e)}",
            "fallback": True,
            "chart_type": "rotation_analysis",
            "description": "AnÃ¡lisis de velocidad de rotaciÃ³n (datos de respaldo)"
        }

# ===== MODIFICAR ENDPOINTS EXISTENTES PARA SOPORTAR FILTROS DE PERÃODO =====
# 3. Modificar pareto-80-20 existente:
@app.get("/products/analytics/pareto-80-20")
async def get_products_pareto_analysis(db: Session = Depends(get_database)):
    """
    GrÃ¡fico de Pareto (80/20): 20% de productos que generan 80% de las ventas
    Variables: Articulo, Venta, participaciÃ³n acumulada
    """
    try:
        # Consulta para anÃ¡lisis de Pareto
        query = text("""
            WITH product_sales AS (
                SELECT 
                    COALESCE(articulo, 'Producto sin nombre') as producto,
                    COALESCE(categoria, 'Sin categorÃ­a') as categoria,
                    SUM(COALESCE(venta, 0)) as total_ventas,
                    SUM(COALESCE(cantidad, 0)) as total_cantidad,
                    SUM(COALESCE(mb, 0)) as total_margen
                FROM client_data 
                WHERE articulo IS NOT NULL AND articulo != ''
                GROUP BY articulo, categoria
            ),
            ranked_products AS (
                SELECT 
                    producto,
                    categoria,
                    total_ventas,
                    total_cantidad,
                    total_margen,
                    ROW_NUMBER() OVER (ORDER BY total_ventas DESC) as ranking,
                    SUM(total_ventas) OVER () as ventas_totales
                FROM product_sales
                WHERE total_ventas > 0
            ),
            pareto_analysis AS (
                SELECT 
                    producto,
                    categoria,
                    total_ventas,
                    total_cantidad,
                    total_margen,
                    ranking,
                    ventas_totales,
                    ROUND((total_ventas / ventas_totales) * 100, 2) as participacion_individual,
                    ROUND((SUM(total_ventas) OVER (ORDER BY ranking) / ventas_totales) * 100, 2) as participacion_acumulada,
                    COUNT(*) OVER () as total_productos
                FROM ranked_products
            )
            SELECT 
                producto,
                categoria,
                total_ventas,
                total_cantidad,
                total_margen,
                ranking,
                participacion_individual,
                participacion_acumulada,
                total_productos,
                CASE 
                    WHEN participacion_acumulada <= 80 THEN 'Top 80%'
                    WHEN participacion_acumulada <= 95 THEN 'Medio 15%'
                    ELSE 'Bottom 5%'
                END as categoria_pareto,
                CASE 
                    WHEN participacion_acumulada <= 80 THEN true
                    ELSE false
                END as es_top_80
            FROM pareto_analysis
            ORDER BY ranking
            LIMIT 200
        """)
        
        result = db.execute(query).fetchall()
        
        data = []
        top_80_count = 0
        total_products = 0
        
        for row in result:
            total_products = row.total_productos
            if row.es_top_80:
                top_80_count += 1
                
            data.append({
                "producto": row.producto,
                "categoria": row.categoria,
                "total_ventas": float(row.total_ventas),
                "total_cantidad": float(row.total_cantidad),
                "total_margen": float(row.total_margen),
                "ranking": row.ranking,
                "participacion_individual": float(row.participacion_individual),
                "participacion_acumulada": float(row.participacion_acumulada),
                "categoria_pareto": row.categoria_pareto,
                "es_top_80": row.es_top_80
            })
        
        # Calcular estadÃ­sticas del Pareto
        pareto_stats = {
            "total_productos": total_products,
            "productos_top_80": top_80_count,
            "porcentaje_productos_top_80": round((top_80_count / total_products) * 100, 1) if total_products > 0 else 0,
            "cumple_regla_80_20": top_80_count <= (total_products * 0.3)  # TÃ­picamente el 20-30% de productos genera el 80%
        }
        
        return {
            "success": True,
            "data": data,
            "pareto_stats": pareto_stats,
            "chart_type": "pareto",
            "description": f"AnÃ¡lisis de Pareto: {top_80_count} productos ({pareto_stats['porcentaje_productos_top_80']}%) generan el 80% de las ventas"
        }
        
    except Exception as e:
        logger.error(f"Error en anÃ¡lisis de Pareto: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))



        # ENDPOINT PARA OBTENER COMERCIALES DEL CSV
# Agregar este endpoint al archivo main.py o al router correspondiente

@app.get("/analytics/comerciales")
async def get_comerciales():
    """
    Obtiene la lista Ãºnica de comerciales del CSV cargado
    """
    try:
        if not hasattr(csv_service, 'data') or csv_service.data is None:
            return {
                "success": False,
                "error": "No hay datos CSV cargados",
                "comerciales": []
            }
        
        df = csv_service.data
        logger.info(f"ðŸ“Š Obteniendo comerciales de {len(df)} registros")
        
        # Buscar columnas que puedan contener informaciÃ³n de comerciales
        possible_columns = [col for col in df.columns 
                          if any(keyword in col.lower() 
                               for keyword in ['comercial', 'vendedor', 'agente', 'seller', 'sales'])]
        
        logger.info(f"ðŸ” Columnas posibles para comerciales: {possible_columns}")
        
        if not possible_columns:
            # Si no encuentra columnas especÃ­ficas, buscar patrones en los datos
            logger.warning("âš ï¸ No se encontraron columnas especÃ­ficas de comerciales")
            return {
                "success": True,
                "comerciales": [],
                "message": "No se encontraron columnas de comerciales en el CSV"
            }
        
        # Usar la primera columna encontrada
        comercial_column = possible_columns[0]
        logger.info(f"ðŸ“‹ Usando columna de comerciales: {comercial_column}")
        
        # Obtener valores Ãºnicos de comerciales
        comerciales_raw = df[comercial_column].dropna().unique().tolist()
        
        # Limpiar y filtrar comerciales
        comerciales_clean = []
        for comercial in comerciales_raw:
            if comercial and str(comercial).strip() and str(comercial).strip() != '':
                comercial_clean = str(comercial).strip()
                if comercial_clean not in comerciales_clean:
                    comerciales_clean.append(comercial_clean)
        
        # Ordenar alfabÃ©ticamente
        comerciales_clean.sort()
        
        logger.info(f"âœ… Encontrados {len(comerciales_clean)} comerciales Ãºnicos")
        logger.info(f"ðŸ“‹ Primeros comerciales: {comerciales_clean[:5]}")
        
        return {
            "success": True,
            "comerciales": comerciales_clean,
            "total_comerciales": len(comerciales_clean),
            "column_used": comercial_column,
            "total_records": len(df)
        }
        
    except Exception as e:
        logger.error(f"âŒ Error obteniendo comerciales: {str(e)}")
        return {
            "success": False,
            "error": f"Error procesando comerciales: {str(e)}",
            "comerciales": []
        }


# MODIFICACIÃ“N AL ENDPOINT DE RECOMENDACIONES ML
# Modificar el endpoint existente de recomendaciones para incluir filtro por comercial

@app.get("/ml/cross-sell-recommendations")
async def get_cross_sell_recommendations_postgresql(
    limit: int = 50,
    min_probability: float = 0.3,
    comercial: Optional[str] = None,  # Filtro por comercial
    db: Session = Depends(get_database)
):
    """Obtener recomendaciones de venta cruzada usando datos REALES del CSV"""
    try:
        logger.info("ðŸ¤– Iniciando recomendaciones ML con datos reales...")
        
        # Query base para obtener datos agregados por cliente
        base_query = """
            SELECT 
                cliente,
                COALESCE(tipo_de_cliente, 'Sin tipo') as tipo_de_cliente,
                COALESCE(comercial, 'Sin asignar') as comercial,
                COALESCE(categoria, 'Sin categorÃ­a') as categoria,
                COALESCE(codigo, 'Sin cÃ³digo') as codigo_cliente,
                COALESCE(proveedor, 'Sin proveedor') as proveedor,
                
                -- MÃ©tricas agregadas
                COUNT(*) as num_transacciones,
                COUNT(DISTINCT factura) as num_facturas,
                SUM(COALESCE(cantidad, 0)) as cantidad_total,
                
                -- Valores monetarios
                ROUND(CAST(SUM(
                    CASE 
                        WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN venta::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as venta_total,
                
                ROUND(CAST(SUM(
                    CASE 
                        WHEN costo IS NOT NULL AND costo::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN costo::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as costo_total,
                
                ROUND(CAST(SUM(
                    CASE 
                        WHEN mb IS NOT NULL AND mb::text ~ '^[0-9]+\.?[0-9]*$' 
                        THEN mb::numeric
                        ELSE 0
                    END
                ) AS NUMERIC), 2) as mb_total,
                
                -- Fechas
                MIN(fecha) as primera_compra,
                MAX(fecha) as ultima_compra
                
            FROM client_data 
            WHERE cliente IS NOT NULL 
            AND TRIM(cliente) != ''
            {comercial_filter}
            GROUP BY cliente, tipo_de_cliente, comercial, categoria, codigo, proveedor
            HAVING SUM(
                CASE 
                    WHEN venta IS NOT NULL AND venta::text ~ '^[0-9]+\.?[0-9]*$' 
                    THEN venta::numeric
                    ELSE 0
                END
            ) > 0
            ORDER BY venta_total DESC
            LIMIT :limit_param
        """
        
        # Aplicar filtro de comercial si se especifica
        comercial_filter = ""
        query_params = {"limit_param": limit}
        
        if comercial and comercial.strip():
            comercial_filter = "AND comercial ILIKE :comercial"
            query_params["comercial"] = f"%{comercial.strip()}%"
        
        final_query = text(base_query.format(comercial_filter=comercial_filter))
        result = db.execute(final_query, query_params).fetchall()
        
        if not result:
            return {
                "success": False,
                "message": f"No se encontraron clientes{' para el comercial ' + comercial if comercial else ''}",
                "recommendations": []
            }
        
        # Convertir datos para ML
        client_data = []
        for row in result:
            client_dict = {
                "id": len(client_data) + 1,
                "cliente": row.cliente,
                "venta": float(row.venta_total or 0),
                "costo": float(row.costo_total or 0),
                "mb": float(row.mb_total or 0),
                "cantidad": float(row.cantidad_total or 0),
                "tipo_de_cliente": row.tipo_de_cliente,
                "categoria": row.categoria,
                "comercial": row.comercial,
                "proveedor": row.proveedor,
                "codigo_cliente": row.codigo_cliente,
                "num_transacciones": row.num_transacciones,
                "num_facturas": row.num_facturas,
                "primera_compra": row.primera_compra,
                "ultima_compra": row.ultima_compra
            }
            client_data.append(client_dict)
        
        # Usar ML service para predicciones
        if not ml_service.is_loaded:
            logger.warning("âš ï¸ Modelo ML no disponible, usando predicciones demo")
        
        all_predictions = ml_service.predict_cross_sell(client_data)
        
        # Filtrar por probabilidad mÃ­nima y enriquecer con datos reales
        filtered_recommendations = []
        for pred in all_predictions:
            if pred['prediction'] == 1 and pred['probability'] >= min_probability:
                # Encontrar datos originales del cliente
                original_data = next((c for c in client_data if c['cliente'] == pred['client_name']), {})
                
                # Enriquecer predicciÃ³n con datos reales
                enriched_pred = {
                    **pred,
                    # Datos reales del CSV
                    "codigo_cliente": original_data.get('codigo_cliente', 'Sin cÃ³digo'),
                    "tipo_cliente": original_data.get('tipo_de_cliente', 'Sin tipo'),  # Corregido
                    "categoria": original_data.get('categoria', 'Sin categorÃ­a'),
                    "comercial": original_data.get('comercial', 'Sin asignar'),
                    "proveedor": original_data.get('proveedor', 'Sin proveedor'),
                    "num_transacciones": original_data.get('num_transacciones', 0),
                    "num_facturas": original_data.get('num_facturas', 0),
                    "primera_compra": original_data.get('primera_compra'),
                    "ultima_compra": original_data.get('ultima_compra'),
                    "cantidad_total": original_data.get('cantidad', 0),
                    "costo_total": original_data.get('costo', 0),
                    "mb_total": original_data.get('mb', 0)
                }
                
                filtered_recommendations.append(enriched_pred)
        
        # Ordenar por probabilidad descendente
        filtered_recommendations.sort(key=lambda x: x['probability'], reverse=True)
        
        logger.info(f"âœ… {len(filtered_recommendations)} recomendaciones generadas con datos reales")
        
        return {
            "success": True,
            "message": f"Recomendaciones generadas usando datos reales del CSV",
            "total_evaluated": len(client_data),
            "total_recommendations": len(filtered_recommendations),
            "filter_comercial": comercial,
            "min_probability_used": min_probability,
            "recommendations": filtered_recommendations,
            "data_source": "CSV real + Modelo ML"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en recomendaciones ML: {str(e)}")
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "recommendations": []
        }

        # AGREGAR ESTE ENDPOINT AL ARCHIVO main.py del backend

@app.get("/analytics/comerciales")
async def get_comerciales_from_csv(db: Session = Depends(get_database)):
    """
    Obtiene la lista Ãºnica de comerciales del CSV cargado
    """
    try:
        logger.info("ðŸ” Obteniendo comerciales del CSV...")
        
        # Verificar que hay datos
        total_records = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar()
        if total_records == 0:
            return {
                "success": False,
                "message": "No hay datos cargados en el sistema",
                "comerciales": []
            }
        
        # Query para obtener comerciales Ãºnicos
        comerciales_query = text("""
            SELECT DISTINCT comercial
            FROM client_data 
            WHERE comercial IS NOT NULL 
            AND TRIM(comercial) != ''
            AND comercial != 'N/A'
            AND comercial != 'Sin asignar'
            ORDER BY comercial
        """)
        
        result = db.execute(comerciales_query).fetchall()
        
        # Procesar lista de comerciales
        comerciales = [row.comercial for row in result if row.comercial]
        
        logger.info(f"âœ… Encontrados {len(comerciales)} comerciales Ãºnicos")
        logger.info(f"ðŸ“‹ Comerciales: {comerciales[:5]}{'...' if len(comerciales) > 5 else ''}")
        
        return {
            "success": True,
            "comerciales": comerciales,
            "total_comerciales": len(comerciales),
            "total_records": total_records,
            "message": f"Se encontraron {len(comerciales)} comerciales Ãºnicos"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error obteniendo comerciales: {str(e)}")
        return {
            "success": False,
            "error": f"Error procesando comerciales: {str(e)}",
            "comerciales": []
        }


        # AGREGAR ESTE ENDPOINT AL ARCHIVO main.py del backend
# (despuÃ©s de los otros endpoints existentes)

@app.get("/analytics/comerciales")
async def get_comerciales_from_csv(db: Session = Depends(get_database)):
    """
    Obtiene la lista Ãºnica de comerciales del CSV cargado
    """
    try:
        logger.info("ðŸ” Obteniendo comerciales del CSV...")
        
        # Verificar que hay datos
        total_records = db.execute(text("SELECT COUNT(*) FROM client_data")).scalar()
        if total_records == 0:
            return {
                "success": False,
                "message": "No hay datos cargados en el sistema",
                "comerciales": []
            }
        
        # Query para obtener comerciales Ãºnicos
        comerciales_query = text("""
            SELECT DISTINCT comercial
            FROM client_data 
            WHERE comercial IS NOT NULL 
            AND TRIM(comercial) != ''
            AND comercial != 'N/A'
            AND comercial != 'Sin asignar'
            AND comercial != 'null'
            AND comercial != 'NULL'
            ORDER BY comercial
        """)
        
        result = db.execute(comerciales_query).fetchall()
        
        # Procesar lista de comerciales
        comerciales = [row.comercial for row in result if row.comercial]
        
        logger.info(f"âœ… Encontrados {len(comerciales)} comerciales Ãºnicos")
        logger.info(f"ðŸ“‹ Comerciales: {comerciales[:5]}{'...' if len(comerciales) > 5 else ''}")
        
        return {
            "success": True,
            "comerciales": comerciales,
            "total_comerciales": len(comerciales),
            "total_records": total_records,
            "message": f"Se encontraron {len(comerciales)} comerciales Ãºnicos"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error obteniendo comerciales: {str(e)}")
        return {
            "success": False,
            "error": f"Error procesando comerciales: {str(e)}",
            "comerciales": []
        }


@app.get("/clients/analytics/sales-by-type-detailed")
async def get_sales_by_type_detailed(db: Session = Depends(get_database)):
    return await get_sales_by_type_detailed_robust(db)

@app.get("/clients/analytics/acquisition-trend")
async def get_acquisition_trend(db: Session = Depends(get_database)):
    return await get_acquisition_trend_fixed_final(db)

@app.get("/clients/analytics/client-type-analysis")
async def get_client_type_analysis(db: Session = Depends(get_database)):
    return await get_client_type_analysis_postgresql(db)

@app.get("/debug/test-acquisition")
async def debug_test_acquisition_endpoint(db: Session = Depends(get_database)):
    return await debug_test_acquisition(db)

# AGREGAR ESTOS ENDPOINTS AL ARCHIVO main.py

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel, EmailStr
from typing import Optional, List
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from auth import (
    get_password_hash, 
    verify_password, 
    create_access_token, 
    authenticate_user,
    validate_email_domain,
    get_current_user,
    get_current_admin_user
)
from models import User, UserRole, UserStatus

# ===== MODELOS PYDANTIC PARA USUARIOS =====

class UserCreate(BaseModel):
    first_name: str
    last_name: str
    email: EmailStr

class UserRegister(BaseModel):
    email: EmailStr
    password: str
    name: str  # Nombre completo para compatibilidad con frontend

class UserUpdate(BaseModel):
    first_name: Optional[str] = None
    last_name: Optional[str] = None
    email: Optional[EmailStr] = None

class UserLogin(BaseModel):
    email: EmailStr
    password: str

class Token(BaseModel):
    access_token: str
    token_type: str
    user: dict

class UserResponse(BaseModel):
    id: int
    first_name: str
    last_name: str
    full_name: str
    email: str
    role: str
    status: str
    is_active: bool
    created_at: Optional[datetime]
    
    class Config:
        from_attributes = True

# ===== ENDPOINTS DE AUTENTICACIÃ“N =====

@app.post("/auth/login", response_model=Token)
async def login(
    login_data: UserLogin,
    db: Session = Depends(get_database)
):
    """
    Login de usuarios (admin y analistas)
    """
    try:
        logger.info(f"ðŸ” Intento de login: {login_data.email}")
        
        # Verificar si es el admin hardcoded
        if login_data.email == "admin@anders.com" and login_data.password == "contra123":
            logger.info("âœ… Login como ADMIN hardcoded")
            
            # Crear token para admin
            access_token = create_access_token(
                data={"sub": login_data.email, "role": "admin"}
            )
            
            return {
                "access_token": access_token,
                "token_type": "bearer",
                "user": {
                    "email": "admin@anders.com",
                    "role": "admin",
                    "full_name": "Administrador",
                    "status": "Activo"
                }
            }
        
        # Autenticar usuario normal desde base de datos
        user = authenticate_user(db, login_data.email, login_data.password)
        
        if not user:
            logger.warning(f"âŒ Login fallido: credenciales incorrectas para {login_data.email}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Email o contraseÃ±a incorrectos"
            )
        
        # Verificar que el usuario estÃ© activo
        if user.status != UserStatus.ACTIVE:
            logger.warning(f"âŒ Login fallido: usuario {login_data.email} no estÃ¡ activo ({user.status})")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Tu cuenta estÃ¡ en estado: {user.status.value}. Contacta al administrador."
            )
        
        # Actualizar Ãºltima fecha de login
        user.last_login = datetime.utcnow()
        db.commit()
        
        # Crear token
        access_token = create_access_token(
            data={"sub": user.email, "role": user.role.value}
        )
        
        logger.info(f"âœ… Login exitoso: {user.email} ({user.role.value})")
        
        return {
            "access_token": access_token,
            "token_type": "bearer",
            "user": user.to_dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error en login: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error en el servidor: {str(e)}"
        )

@app.post("/auth/register")
async def register_user(
    register_data: UserRegister,
    db: Session = Depends(get_database)
):
    """
    Registro de nuevos analistas
    Solo pueden registrarse si fueron previamente agregados por un admin
    """
    try:
        logger.info(f"ðŸ“ Intento de registro: {register_data.email}")
        
        # Validar dominio de email
        if not validate_email_domain(register_data.email):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Solo se permiten correos @anders.com"
            )
        
        # Verificar que el usuario existe en la base de datos
        user = db.query(User).filter(User.email == register_data.email.lower()).first()
        
        if not user:
            logger.warning(f"âŒ Email {register_data.email} no autorizado")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Este correo no estÃ¡ autorizado. Contacta al administrador."
            )
        
        # Verificar que no tenga contraseÃ±a ya configurada
        if user.hashed_password:
            logger.warning(f"âŒ Usuario {register_data.email} ya estÃ¡ registrado")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Este correo ya estÃ¡ registrado. Por favor, inicia sesiÃ³n."
            )
        
        # Validar contraseÃ±a
        if len(register_data.password) < 6:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="La contraseÃ±a debe tener al menos 6 caracteres"
            )
        
        # Actualizar usuario con contraseÃ±a
        user.hashed_password = get_password_hash(register_data.password)
        user.status = UserStatus.ACTIVE
        user.is_active = True
        user.updated_at = datetime.utcnow()
        
        db.commit()
        db.refresh(user)
        
        logger.info(f"âœ… Usuario registrado exitosamente: {user.email}")
        
        return {
            "success": True,
            "message": "Registro exitoso. Ahora puedes iniciar sesiÃ³n.",
            "user": user.to_dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ Error en registro: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error en el servidor: {str(e)}"
        )

# ===== ENDPOINTS DE GESTIÃ“N DE ANALISTAS (SOLO ADMIN) =====

@app.get("/users/analysts", response_model=List[UserResponse])
async def get_analysts(
    current_user: User = Depends(get_current_admin_user),
    db: Session = Depends(get_database)
):
    """
    Obtener lista de todos los analistas (solo admin)
    """
    try:
        analysts = db.query(User).filter(User.role == UserRole.ANALYST).all()
        return [user.to_dict() for user in analysts]
    except Exception as e:
        logger.error(f"Error obteniendo analistas: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/users/analysts")
async def get_analysts(
    current_user: User = Depends(get_current_admin_user),
    db: Session = Depends(get_database)
):
    """
    Obtener lista de todos los analistas (solo admin)
    """
    try:
        logger.info(f"ðŸ‘¤ Usuario {current_user.email if hasattr(current_user, 'email') else 'admin'} solicitando analistas")
        
        # Obtener analistas
        analysts = db.query(User).filter(User.role == UserRole.ANALYST).all()
        
        logger.info(f"âœ… Encontrados {len(analysts)} analistas")
        
        # Convertir a dict
        analysts_data = []
        for analyst in analysts:
            try:
                analysts_data.append({
                    "id": analyst.id,
                    "first_name": analyst.first_name,
                    "last_name": analyst.last_name,
                    "full_name": f"{analyst.first_name} {analyst.last_name}",
                    "email": analyst.email,
                    "role": analyst.role.value if analyst.role else "analyst",
                    "status": analyst.status.value if analyst.status else "Inactivo",
                    "is_active": analyst.is_active,
                    "created_at": analyst.created_at.isoformat() if analyst.created_at else None,
                    "last_login": analyst.last_login.isoformat() if analyst.last_login else None
                })
            except Exception as e:
                logger.error(f"Error convirtiendo analista {analyst.id}: {str(e)}")
        
        return {
            "success": True,
            "analysts": analysts_data,
            "total": len(analysts_data)
        }
        
    except Exception as e:
        logger.error(f"âŒ Error obteniendo analistas: {str(e)}")
        return {
            "success": False,
            "message": str(e),
            "analysts": []
        }

@app.put("/users/analysts/{analyst_id}")
async def update_analyst(
    analyst_id: int,
    user_data: UserUpdate,
    current_user: User = Depends(get_current_admin_user),
    db: Session = Depends(get_database)
):
    """
    Actualizar un analista (solo admin)
    """
    try:
        analyst = db.query(User).filter(User.id == analyst_id).first()
        
        if not analyst:
            raise HTTPException(status_code=404, detail="Analista no encontrado")
        
        # Actualizar campos
        if user_data.first_name:
            analyst.first_name = user_data.first_name
        if user_data.last_name:
            analyst.last_name = user_data.last_name
        if user_data.email:
            if not validate_email_domain(user_data.email):
                raise HTTPException(
                    status_code=400,
                    detail="El correo debe terminar con @anders.com"
                )
            # Verificar que el nuevo email no exista
            existing = db.query(User).filter(
                User.email == user_data.email.lower(),
                User.id != analyst_id
            ).first()
            if existing:
                raise HTTPException(status_code=400, detail="Este correo ya estÃ¡ en uso")
            
            analyst.email = user_data.email.lower()
        
        analyst.updated_at = datetime.utcnow()
        
        db.commit()
        db.refresh(analyst)
        
        logger.info(f"âœ… Analista actualizado: {analyst.email}")
        
        return {
            "success": True,
            "message": "Analista actualizado exitosamente",
            "analyst": analyst.to_dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"Error actualizando analista: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/users/analysts/{analyst_id}")
async def delete_analyst(
    analyst_id: int,
    current_user: User = Depends(get_current_admin_user),
    db: Session = Depends(get_database)
):
    """
    Eliminar un analista (solo admin)
    """
    try:
        analyst = db.query(User).filter(User.id == analyst_id).first()
        
        if not analyst:
            raise HTTPException(status_code=404, detail="Analista no encontrado")
        
        db.delete(analyst)
        db.commit()
        
        logger.info(f"âœ… Analista eliminado: {analyst.email}")
        
        return {
            "success": True,
            "message": "Analista eliminado exitosamente"
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"Error eliminando analista: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/users/me")
async def get_current_user_info(current_user: User = Depends(get_current_user)):
    """
    Obtener informaciÃ³n del usuario actual
    """
    return current_user.to_dict()

# Para desarrollo local
# if __name__ == "__main__":
#     import uvicorn
#     import os
#     port = int(os.environ.get("PORT", 8000))
#     uvicorn.run(app, host="0.0.0.0", port=port)